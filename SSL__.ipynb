{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-Supervised Learning (SSL)\n",
    "\n",
    "\n",
    "SSL studies how to learn from both labeled and unlabeled data, which can be useful when data is abundant but the resources to label them are limited.\n",
    "\n",
    "In this exercise, you will:\n",
    "\n",
    "* Given a simulated dataset with both labeled and unlabeled data, build a similarity graph and use the Harmonic Function Solution (HSF) to predict the labels of the unlabeled data;\n",
    "* Use HSF for face recognition, given a fixed dataset;\n",
    "* Implement an online version of HSF to label images as they appear in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Harmonic Function Solution\n",
    "\n",
    "Let $G = (V, E)$ be a weighted undirected graph where $V = \\{x_1, \\ldots, x_n \\}$ is the vertex set and $E$ is the edge set. Each edge $e_{ij} \\in E$ has a weight $w_{ij}$ and, if there is no edge between $x_i$ and $x_j$, then $w_{ij}=0$.\n",
    "\n",
    "Let $|V| = n$ be the total number of nodes. Only a subset of the nodes $S \\subset V$ with cardinality $|S| = l$ is labeled, and the remaining $u = n - l$ nodes are placed in the subset $T = V \\setminus S$. \n",
    "\n",
    "Our goal is to predict the labels of the vertices in $T$ using the structure of the graph. Since we believe that nodes close in the graph should have similar labels, we would like to have each node surrounded by a majority of nodes with the same label. In order to do so, we impose that the labeling vector $f \\in \\mathbb{R}^n$ must be an **harmonic function** on the graph, that is:\n",
    "\n",
    "$$\n",
    "f_i = \\frac{\\sum_{j} w_{ij} f_j}{\\sum_{j} w_{ij}},  \\forall i \\in T\n",
    "$$\n",
    "\n",
    "One interpretation for this constraint is that $w_{ij}$ represents the tendency of moving from node $x_i$ to node $x_j$, the stationary distribution of the transition matrix $P(j|i) = \\tfrac{w_{ij}}{\\sum_{k} w_{ik}}$  is a valid solution to our problem. \n",
    "\n",
    "### Hard HFS\n",
    "\n",
    "It can be shown that $f$ is harmonic if and only if $(Lf)_T = 0$, where $(Lf)_T$ is the vector containing the values of $Lf$ for the nodes in the set $T$, and $L$ is the graph Laplacian. \n",
    "\n",
    "Hence, the harmonic function solution to the SSL problem is the solution to the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{f \\in \\mathbb{R}^n}  f^T L f  \n",
    "\\quad \\text{s.t} \\quad\n",
    "y_i = f(x_i) \\quad \\forall x_i \\in S\n",
    "$$\n",
    "where $y_i$ are the labels available for the vertices $x_i \\in S$. This gives us:\n",
    "\n",
    "$$\n",
    "f_T = L_{TT}^{-1}(W_{TS}f_S) = - L_{TT}^{-1}(L_{TS}f_S) \n",
    "$$\n",
    "\n",
    "### Soft HFS\n",
    "\n",
    "If the labels are noisy, we might need to replace the \"hard\" constraint of the optimization problem above by a \"soft\" constraint. Let $C$ be a diagonal matrix such that $C_{ii} = c_l$ for labeled examples and $C_{ii} = c_u$ otherwise. Also, define $y_i = 0$ for unlabeled examples, that is, for $x_i \\in T$. \n",
    "\n",
    "The soft HFS objective function is\n",
    "\n",
    "$$\n",
    "\\min_{f\\in\\mathbb{R}^n} (f-y)^T C (f-y) + f^T L f\n",
    "$$\n",
    "whose solution is \n",
    "\n",
    "$$\n",
    "f^* = (C^{-1}L+I)^{-1}y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Implement hard and soft HFS in the function `compute_hfs`. Complete the function `two_moons_hfs` to test your implementation using the datasets `data_2moons_hfs.mat` and `data_2moons_hfs_large.mat`.\n",
    "\n",
    "\n",
    "* Tips: \n",
    "    * Don't forget to choose well the parameters to build the graph and its Laplacian.\n",
    "    * You can use the functions `build_laplacian_regularized` and `build_similarity_graph`. The function `mask_labels` is used to chose how many labels are revealed.\n",
    "    * Be careful: the labels are revealed randomly, and each random realization can have different results! Check how the `seed` parameter works.\n",
    "    * Introduce noisy labels to compare hard and soft HFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install  -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.spatial.distance as sd\n",
    "from scipy.io import loadmat\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('ssl/')\n",
    "from helper import build_similarity_graph, label_noise\n",
    "from helper import build_laplacian, build_laplacian_regularized\n",
    "from helper import plot_classification\n",
    "from helper import mask_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define parameters for HFS\n",
    "\"\"\"\n",
    "params = {}\n",
    "\n",
    "# regularization parameter (gamma)\n",
    "params['laplacian_regularization'] = 0.0\n",
    "\n",
    "# the sigma value for the exponential (similarity) function, already squared\n",
    "params['var'] = 1.0\n",
    "\n",
    "# Threshold eps for epsilon graphs\n",
    "params['eps'] = .5\n",
    "\n",
    "# Number of neighbours k for k-nn. If zero, use epsilon-graph\n",
    "params['k'] = 0\n",
    "\n",
    "# String selecting which version of the laplacian matrix to construct.\n",
    "# 'unn':  unnormalized, 'sym': symmetric normalization, 'rw':  random-walk normalization \n",
    "params['laplacian_normalization'] = 'unn'\n",
    "\n",
    "# Coefficients for C matrix for soft HFS\n",
    "params['c_l'] = 1.\n",
    "params['c_u'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hfs(L, Y, soft=False, **params):\n",
    "    \"\"\"\n",
    "    TO BE COMPLETED\n",
    "\n",
    "    Function to perform HFS (hard or soft!).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    L : array\n",
    "        Graph Laplacian, (n x n) matrix (regularized or not)\n",
    "    Y : array\n",
    "        (n, ) array with nodes labels [0, 1, ... , num_classes] (0 is unlabeled)\n",
    "    soft : bool\n",
    "        If True, compute soft HFS. Otherwise, compute hard HFS.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "        Labels, class assignments for each of the n nodes\n",
    "    \"\"\"\n",
    "\n",
    "    num_samples = L.shape[0]\n",
    "    Cl = np.unique(Y)\n",
    "    num_classes = len(Cl)-1\n",
    "\n",
    "    \"\"\"\n",
    "    Build the vectors:\n",
    "    y = (n x num_classes) target vector \n",
    "    l_idx = shape (l,) vector with indices of labeled nodes\n",
    "    u_idx = shape (u,) vector with indices of unlabeled nodes\n",
    "    \"\"\"\n",
    "    # ...\n",
    "    # one_hot encoding removing class 0.\n",
    "    array_for_one_hot = np.eye(num_classes + 1 )\n",
    "    array_for_one_hot[0,0] -= 1\n",
    "    y = array_for_one_hot[np.array(Y,dtype=int) ][:,1:] # y is a concatenation of lines of array_for_one_hot \n",
    "\n",
    "    l_idx = np.array([ ind for ind in range(num_samples) if Y[ind] > 0 ])\n",
    "    u_idx = np.array([ ind for ind in range(num_samples) if Y[ind] == 0 ])\n",
    "\n",
    "    \n",
    "    if not soft:    \n",
    "        \"\"\"\n",
    "        Compute hard HFS.  \n",
    "\n",
    "        f_l = solution for labeled data. \n",
    "        f_u = solution for unlabeled data\n",
    "        f   = solution for all data\n",
    "        \"\"\"\n",
    "\n",
    "        f_l = y[l_idx]\n",
    "        \n",
    "        Luu =  L[u_idx][:, u_idx]\n",
    "        Lul =  L[u_idx][:, l_idx]\n",
    "        f_u = - np.linalg.inv(Luu) @ Lul @ f_l\n",
    "\n",
    "        f = np.zeros((num_samples, num_classes))\n",
    "        f[l_idx] = f_l\n",
    "        f[u_idx] = f_u\n",
    "\n",
    "\n",
    "    else:\n",
    "        \"\"\"\n",
    "        Compute soft HFS.\n",
    "        f = harmonic function solution \n",
    "        C = (n x n) diagonal matrix with c_l for labeled samples and c_u otherwise    \n",
    "        \"\"\"\n",
    "        f = None\n",
    "        C = np.identity(num_samples)\n",
    "        C[l_idx] *= params['c_l']\n",
    "        C[u_idx] *= params['c_u']\n",
    "        \n",
    "        f = np.linalg.inv(np.linalg.inv(C) @ L + np.identity(num_samples)) @ y\n",
    "\n",
    "    \"\"\"\n",
    "    return the labels assignment from the hfs solution, and the solution f\n",
    "    labels: (n x 1) class assignments [1,2,...,num_classes]    \n",
    "    f : harmonic function solution\n",
    "    \"\"\"\n",
    "    labels = np.argmax(f, axis = -1) + 1\n",
    "    return labels, f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_moons_hfs(l=4, l_noisy=1, soft=False, dataset='data_2moons_hfs.mat', plot=True, seed=None, **params):\n",
    "    \"\"\"    \n",
    "    TO BE COMPLETED.\n",
    "\n",
    "    HFS for two_moons data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    l : int\n",
    "        Number of labeled (unmasked) nodes provided to the HFS algorithm.\n",
    "    l_noisy : int\n",
    "        Number of *noisy* labels to introduce.\n",
    "    soft : bool\n",
    "        If true, use soft HFS, otherwise use hard HFS\n",
    "    dataset : {'data_2moons_hfs.mat' or 'data_2moons_hfs_large.mat'}\n",
    "        Which dataset to use.\n",
    "    plot : bool\n",
    "        If True, show plots\n",
    "    seed : int\n",
    "        If not None, set global numpy seed before choosing labels to reveal.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Load the data. At home, try to use the larger dataset.    \n",
    "    in_data = loadmat(os.path.join('data', dataset))\n",
    "    X = in_data['X']\n",
    "    Y = np.array(in_data['Y'].squeeze(), dtype=np.uint32)\n",
    "\n",
    "    # infer number of labels from samples\n",
    "    num_samples = np.size(Y, 0)\n",
    "    unique_classes = np.unique(Y)\n",
    "    num_classes = len(unique_classes)\n",
    "    \n",
    "    # mask labels\n",
    "    Y_masked = mask_labels(Y, l)\n",
    "    assert len(np.unique(Y_masked)) > 2, \"only one class in training data!\"\n",
    "    # introduce noise\n",
    "    noise_indices = np.where(Y_masked == 0)[0]\n",
    "    np.random.shuffle(noise_indices)\n",
    "    noise_indices = noise_indices[:l_noisy]\n",
    "    Y_masked[noise_indices] = np.random.choice(unique_classes, l_noisy)\n",
    "\n",
    "    \"\"\"\n",
    "    compute hfs solution using either soft_hfs or hard_hfs\n",
    "    \"\"\"\n",
    "    # Build graph Laplacian using the parameters:\n",
    "    # params['laplacian_regularization'], params['var'], params['eps'], \n",
    "    # params['k'] and params['laplacian_normalization'].\n",
    "    \n",
    "    W = build_similarity_graph(X, var = params['var'],eps = params['eps'], k = params['k'])\n",
    "    L = build_laplacian(W, laplacian_normalization=params['laplacian_normalization'])\n",
    "\n",
    "    labels, f = compute_hfs(L, Y_masked, soft, **params)\n",
    "\n",
    "    # Visualize results\n",
    "    if plot:\n",
    "        plot_classification(X, Y, Y_masked, noise_indices, labels, params['var'], params['eps'], params['k'])\n",
    "    accuracy = np.mean(labels == np.squeeze(Y))\n",
    "    print(f\"Soft={soft}, Accuracy={accuracy}\")\n",
    "    return X, Y, labels, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hrv_or_ecgs_hfs(X, Y, l=4, l_noisy=1, soft=False, seed=None, per_class=True, **params):\n",
    "    \"\"\"    \n",
    "    TO BE COMPLETED.\n",
    "\n",
    "    HFS for two_moons data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    l : int\n",
    "        Number of labeled (unmasked) nodes provided to the HFS algorithm.\n",
    "    l_noisy : int\n",
    "        Number of *noisy* labels to introduce.\n",
    "    soft : bool\n",
    "        If true, use soft HFS, otherwise use hard HFS\n",
    "    dataset : {'data_2moons_hfs.mat' or 'data_2moons_hfs_large.mat'}\n",
    "        Which dataset to use.\n",
    "    plot : bool\n",
    "        If True, show plots\n",
    "    seed : int\n",
    "        If not None, set global numpy seed before choosing labels to reveal.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Load the data. At home, try to use the larger dataset.    \n",
    "    X = X\n",
    "    Y = np.array(Y.squeeze(), dtype=np.uint32)\n",
    "\n",
    "    # infer number of labels from samples\n",
    "    num_samples = np.size(Y, 0)\n",
    "    unique_classes = np.unique(Y)\n",
    "    num_classes = len(unique_classes)\n",
    "    \n",
    "    # mask labels\n",
    "    Y_masked = mask_labels(Y, l, per_class=per_class)\n",
    "    assert len(np.unique(Y_masked)) > 2, \"only one class in training data!\"\n",
    "    # introduce noise\n",
    "    noise_indices = np.where(Y_masked == 0)[0]\n",
    "    np.random.shuffle(noise_indices)\n",
    "    noise_indices = noise_indices[:l_noisy]\n",
    "    Y_masked[noise_indices] = np.random.choice(unique_classes, l_noisy)\n",
    "\n",
    "    \"\"\"\n",
    "    compute hfs solution using either soft_hfs or hard_hfs\n",
    "    \"\"\"\n",
    "    # Build graph Laplacian using the parameters:\n",
    "    # params['laplacian_regularization'], params['var'], params['eps'], \n",
    "    # params['k'] and params['laplacian_normalization'].\n",
    "    \n",
    "    W = build_similarity_graph(X, var = params['var'],eps = params['eps'], k = params['k'])\n",
    "    L = build_laplacian(W, laplacian_normalization=params['laplacian_normalization'])\n",
    "\n",
    "    labels, f = compute_hfs(L, Y_masked, soft, **params)\n",
    "\n",
    "    # # Visualize results\n",
    "    # if plot:\n",
    "    #     plot_classification(X, Y, Y_masked, noise_indices, labels, params['var'], params['eps'], params['k'])\n",
    "    accuracy = np.mean(labels == np.squeeze(Y))\n",
    "    print(f\"Soft={soft}, Accuracy={accuracy}\")\n",
    "    # return X, Y, labels, accuracy\n",
    "    return labels, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from transformations import DataTransform,TransformationRegistry, IdentityTransform, FourierTransform, LowFourierTransform, LowPsdTransform, WaveDecTransform, DwtTransform, CwtTransform, AutoRegTransform, ShannonEncodingTransform, WaveletLeadersTransform, CrossCorTransform, AutoCorTransform, MultiFracsTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the registry\n",
    "registry = TransformationRegistry()\n",
    "\n",
    "# Register transformations\n",
    "registry.register('identity', IdentityTransform)\n",
    "registry.register('fourier', FourierTransform)\n",
    "registry.register('low_fourier', LowFourierTransform)\n",
    "registry.register('low_psd', LowPsdTransform)\n",
    "registry.register('wavedec', WaveDecTransform)\n",
    "registry.register('dwt', DwtTransform)\n",
    "registry.register('cwt', CwtTransform)\n",
    "registry.register('autoreg', AutoRegTransform)\n",
    "registry.register('shannon_encoding', ShannonEncodingTransform)\n",
    "registry.register('wavelet_leaders', WaveletLeadersTransform)\n",
    "registry.register('multifracs', MultiFracsTransform)\n",
    "registry.register('crosscor', CrossCorTransform)\n",
    "registry.register('autocor', AutoCorTransform)\n",
    "\n",
    "# Initialize the data transformer\n",
    "data_transformer = DataTransform(registry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecgs_labels = np.load('ecgs_labels.npy')\n",
    "\n",
    "X_true, Y_true = ecgs_labels[:,:-1], ecgs_labels[:,-1] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_X = np.abs(np.fft.fft(X_true, n=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computating  multifracs{'j2': 12} ..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANT\\AppData\\Roaming\\Python\\Python311\\site-packages\\pymultifracs\\utils.py:87: RuntimeWarning: divide by zero encountered in power\n",
      "  return np.power(array, exponent)\n",
      "C:\\Users\\ANT\\AppData\\Roaming\\Python\\Python311\\site-packages\\pymultifracs\\utils.py:76: RuntimeWarning: divide by zero encountered in reciprocal\n",
      "  return array ** exponent\n",
      "C:\\Users\\ANT\\AppData\\Roaming\\Python\\Python311\\site-packages\\pymultifracs\\cumulants.py:265: RuntimeWarning: divide by zero encountered in log\n",
      "  log_T_X_j = np.log(T_X_j)\n",
      "C:\\Users\\ANT\\AppData\\Roaming\\Python\\Python311\\site-packages\\pymultifracs\\mfspectrum.py:117: RuntimeWarning: invalid value encountered in divide\n",
      "  R_j = temp / Z\n",
      "C:\\Users\\ANT\\AppData\\Roaming\\Python\\Python311\\site-packages\\pymultifracs\\mfspectrum.py:118: RuntimeWarning: divide by zero encountered in log2\n",
      "  V[:, ind_j, :] = fixednansum(R_j * np.log2(mrq_values_j), axis=1)\n",
      "C:\\Users\\ANT\\AppData\\Roaming\\Python\\Python311\\site-packages\\pymultifracs\\mfspectrum.py:118: RuntimeWarning: invalid value encountered in multiply\n",
      "  V[:, ind_j, :] = fixednansum(R_j * np.log2(mrq_values_j), axis=1)\n",
      "C:\\Users\\ANT\\AppData\\Roaming\\Python\\Python311\\site-packages\\pymultifracs\\mfspectrum.py:119: RuntimeWarning: divide by zero encountered in log2\n",
      "  U[:, ind_j, :] = np.log2(nj) + fixednansum((R_j * np.log2(R_j)),\n",
      "C:\\Users\\ANT\\AppData\\Roaming\\Python\\Python311\\site-packages\\pymultifracs\\mfspectrum.py:119: RuntimeWarning: invalid value encountered in multiply\n",
      "  U[:, ind_j, :] = np.log2(nj) + fixednansum((R_j * np.log2(R_j)),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    }
   ],
   "source": [
    "transformed_X = data_transformer.apply_transformation(X_true, 'multifracs', j2=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the classifiers to be tested\n",
    "# classifiers = {\n",
    "#     'SVM': SVC(),\n",
    "#     'DecisionTree': DecisionTreeClassifier(),\n",
    "#     'RandomForest': RandomForestClassifier()\n",
    "# }\n",
    "\n",
    "# # Define the transformations to be tested\n",
    "# transformations = [\n",
    "#     # ['identity'],\n",
    "#     ['crosscor'],\n",
    "#     ['autocor', {'m':5000,'k':4}],\n",
    "#     ['fourier', {'new_dimension':40}],\n",
    "#     ['low_fourier'],\n",
    "#     ['low_psd'],\n",
    "#     ['cwt',{'pca_components' : 10}],\n",
    "#     ['wavedec'],\n",
    "#     ['autoreg', {'k': 3}],\n",
    "#     ['shannon_encoding'],\n",
    "#     ['wavelet_leaders'],\n",
    "#     ['multifracs'],\n",
    "#     ['multifracs', {'j1':1,'j2':12}],\n",
    "#     [['wavelet_leaders','shannon_encoding']],\n",
    "#     [['wavelet_leaders','multifracs']],\n",
    "#     [['fourier','multifracs',], {'new_dimension':40}],\n",
    "#     [['fourier','multifracs',], {'new_dimension':40}],\n",
    "#     [['fourier','multifracs','shannon_encoding'], {'new_dimension':40}],\n",
    "#     [['low_fourier','multifracs','autoreg'], {'k':3}],\n",
    "    \n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Function to evaluate a classifier using cross-validation\n",
    "# def evaluate_classifier_cv(classifier, X, y):\n",
    "#     scores = cross_val_score(classifier, X, y, cv=5)  # 5-fold cross-validation\n",
    "#     return np.mean(scores), np.std(scores)\n",
    "\n",
    "# # Loop over each transformation and each classifier\n",
    "# results = {}\n",
    "\n",
    "# for trans_names in transformations:\n",
    "#     # print()\n",
    "#     trans_names_str = [str(name) for name in trans_names]\n",
    "#     trans_name_str = '+'.join(trans_names_str) if isinstance(trans_names, list) else trans_names\n",
    "#     kwargs = trans_names[1] if isinstance(trans_names, list) and len(trans_names) > 1 else {}\n",
    "#     trans_names = trans_names[0] if isinstance(trans_names, list) else trans_names\n",
    "    \n",
    "#     # Apply transformation\n",
    "#     transformed_X = data_transformer.apply_transformation(X, trans_names, **kwargs)\n",
    "#     print(f\"Transformation: {trans_name_str}, Shape: {transformed_X.shape}\" )\n",
    "#     # Standardize the data (important for some classifiers like SVM)\n",
    "#     scaler = StandardScaler()\n",
    "#     transformed_X = scaler.fit_transform(transformed_X)\n",
    "    \n",
    "#     results[trans_name_str] = {}\n",
    "#     for clf_name, clf in classifiers.items():\n",
    "#         # Evaluate the classifier with cross-validation\n",
    "#         mean_accuracy, std_accuracy = evaluate_classifier_cv(clf, transformed_X, y)\n",
    "#         results[trans_name_str][clf_name] = (mean_accuracy, std_accuracy)\n",
    "#         print(f\"Transformation: {trans_name_str}, Classifier: {clf_name}, Mean Accuracy: {mean_accuracy:.3f}, Std Dev: {std_accuracy:.3f}\")\n",
    "\n",
    "#     print()\n",
    "# # Print the results\n",
    "# for trans_name, clf_results in results.items():\n",
    "#     print()\n",
    "#     for clf_name, (mean_accuracy, std_accuracy) in clf_results.items():\n",
    "#         print(f\"Transformation: {trans_name}, Classifier: {clf_name}, Mean Accuracy: {mean_accuracy:.3f}, Std Dev: {std_accuracy:.3f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft=True, Accuracy=0.9407725321888412\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "hard_labels, hard_accuracy = hrv_or_ecgs_hfs(transformed_X, Y_true, l=210, l_noisy=0, soft=True,\n",
    "                                                 seed=seed, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1 - Report the accuracy you obtained for `data_2moons_hfs.mat` dataset using hard HFS, when l=10 and l_noisy=0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 42\n",
    "# X, Y, hard_labels, hard_accuracy = two_moons_hfs(l=10, l_noisy=0, soft=False, dataset='data_2moons_hfs.mat',\n",
    "#                                                  plot=True, seed=seed, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2  - Using `data_2moons_hfs_large.mat`, run `two_moons_hfs` several times with l=4. What can go wrong?\n",
    "\n",
    "* Tips:\n",
    "    * When running `two_moons_hfs` several times, don't forget to set `seed=None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "# for ii in range(20):\n",
    "#     X, Y, hard_labels, hard_accuracy = two_moons_hfs(l=4, l_noisy=0, soft=False, \n",
    "#                                                      dataset='data_2moons_hfs_large.mat',\n",
    "#                                                      plot=False, seed=None, **params)\n",
    "# mask_labels?  # check parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, there are only examples of one class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3 - Using `data_2moons_hfs.mat`, l=10 and l_noisy=5, compare hard HFS to soft HFS. Report the accuracy and comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\data_2moons_hfs.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\io\\matlab\\_mio.py:39\u001b[0m, in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\data_2moons_hfs.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m plot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \n\u001b[0;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_2moons_hfs.mat\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Try also 'data_2moons_hfs_large.mat'\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m X, Y, hard_labels, hard_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtwo_moons_hfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_noisy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mplot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m X, Y, soft_labels, soft_accuracy \u001b[38;5;241m=\u001b[39m two_moons_hfs(l\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, l_noisy\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, soft\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m      9\u001b[0m                                                  plot\u001b[38;5;241m=\u001b[39mplot, seed\u001b[38;5;241m=\u001b[39mseed, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "Cell \u001b[1;32mIn[24], line 26\u001b[0m, in \u001b[0;36mtwo_moons_hfs\u001b[1;34m(l, l_noisy, soft, dataset, plot, seed, **params)\u001b[0m\n\u001b[0;32m     23\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Load the data. At home, try to use the larger dataset.    \u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m in_data \u001b[38;5;241m=\u001b[39m \u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m X \u001b[38;5;241m=\u001b[39m in_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     28\u001b[0m Y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(in_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint32)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\io\\matlab\\_mio.py:225\u001b[0m, in \u001b[0;36mloadmat\u001b[1;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03mLoad MATLAB file.\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m variable_names \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariable_names\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 225\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_open_file_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmat_reader_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatfile_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mMR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\io\\matlab\\_mio.py:17\u001b[0m, in \u001b[0;36m_open_file_context\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_context\u001b[39m(file_like, appendmat, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     f, opened \u001b[38;5;241m=\u001b[39m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\scipy\\io\\matlab\\_mio.py:45\u001b[0m, in \u001b[0;36m_open_file\u001b[1;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m appendmat \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_like\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     44\u001b[0m         file_like \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReader needs file name or open file-like object\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     49\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\data_2moons_hfs.mat'"
     ]
    }
   ],
   "source": [
    "# Comparing\n",
    "seed = 5  # To run several times with random outcomes, set seed=None. Otherwise, set a seed for reproducibility.\n",
    "plot = True \n",
    "dataset = 'data_2moons_hfs.mat' # Try also 'data_2moons_hfs_large.mat'\n",
    "\n",
    "X, Y, hard_labels, hard_accuracy = two_moons_hfs(l=10, l_noisy=5, soft=False, dataset=dataset,\n",
    "                                                 plot=plot, seed=seed, **params)\n",
    "X, Y, soft_labels, soft_accuracy = two_moons_hfs(l=10, l_noisy=5, soft=True, dataset=dataset,\n",
    "                                                 plot=plot, seed=seed, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is generally better for soft HFS. Noisy labels often result in unnatural labeling for Hard HFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Face recognition with HFS\n",
    "\n",
    "Now, we apply HFS to the task of face recognition, that is, our goal is to classify faces as belonging to different people. Since faces all share common features, it can be a good idea to leverage a large quantity of unlabeled data to improve classification accuracy. In this part of the exercise, you will:\n",
    "\n",
    "* Extract faces from the images using OpenCV for face detection, and use the same library to apply preprocessing steps;\n",
    "* Run HFS for classification.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Choose the hyperparameters and run HFS for face recognition, using both the small and large dataset. You can try to change the preprocessing steps (e.g. equalizeHist, GaussianBlur) applied to the images.\n",
    "\n",
    "**Important**: make sure your HFS code is able to handle more than two classes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1 - How did you manage to label more than two classes?\n",
    "\n",
    "We use one_hot encoding.\n",
    "\n",
    "### Question 2.2 - Report the best accuracy you obtained for both (small and augmented) datasets.\n",
    "\n",
    "* Tips:\n",
    "    * The small dataset (10 images per person) is loaded with `load_image_data`.\n",
    "    * Use `load_image_data_augmented` for the augmented dataset (50 images per person). \n",
    "\n",
    "### Question 2.3 - If the accuracy changes when using the augmented dataset, explain why. Does using additional data always increase the performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ssl' has no attribute 'SSLWantReadError'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from load_images import load_image_data, plot_image_data\n",
    "from load_images import load_image_data_augmented, plot_image_data_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ssl' has no attribute 'SSLWantReadError'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Define parameters for face recognition with HFS\n",
    "\"\"\"\n",
    "params_face_rec = {}\n",
    "params_face_rec['laplacian_regularization'] = 1.0\n",
    "params_face_rec['var'] = 10000.0\n",
    "params_face_rec['eps'] = None\n",
    "params_face_rec['k'] = 20\n",
    "params_face_rec['laplacian_normalization'] = 'unn'\n",
    "params_face_rec['c_l'] = 1\n",
    "params_face_rec['c_u'] = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ssl' has no attribute 'SSLWantReadError'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Function to preprocess the images\n",
    "# You may try to change it and check the impact on the classification accuracy\n",
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : array\n",
    "        (width, height) array representing a grayscale image\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        (96, 96) preprocessed image\n",
    "    \"\"\"\n",
    "    output_frame_size = 96   # do not change the output frame size!\n",
    "    image = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "    image = cv2.equalizeHist(image)\n",
    "    image = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    im = cv2.resize(image, (output_frame_size, output_frame_size)).astype(float)\n",
    "    im -= im.mean()\n",
    "    im /= im.max()\n",
    "    image = im\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ssl' has no attribute 'SSLWantReadError'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 10 images per person\n",
    "np.random.seed(456)   # set seed, since labels are masked randomly\n",
    "images, labels, masked_labels = load_image_data(preprocess_image)\n",
    "\n",
    "# # 50 images per person\n",
    "images_a, labels_a, masked_labels_a = load_image_data_augmented(preprocess_image)\n",
    "plot_image_data_augmented(images_a)\n",
    "\n",
    "# Uncomment below if you want to visualize the images\n",
    "plot_image_data(images)\n",
    "print(images.shape)\n",
    "print(masked_labels.reshape(-1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ssl' has no attribute 'SSLWantReadError'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "params_face_rec['eps'] = 0.8\n",
    "params_face_rec['k'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ssl' has no attribute 'SSLWantReadError'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# graph Laplacian\n",
    "L = build_laplacian_regularized(images, \n",
    "                                params_face_rec['laplacian_regularization'], \n",
    "                                params_face_rec['var'], \n",
    "                                params_face_rec['eps'], \n",
    "                                params_face_rec['k'], \n",
    "                                params_face_rec['laplacian_normalization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ssl' has no attribute 'SSLWantReadError'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Run HFS\n",
    "predicted_labels, f = compute_hfs(L, masked_labels, soft=True, **params_face_rec)\n",
    "print(f.shape)\n",
    "print(predicted_labels.shape)\n",
    "print(f.argmax(axis=-1).shape)\n",
    "accuracy = np.equal(predicted_labels, labels).mean()\n",
    "print(\"Accuracy = \", accuracy)\n",
    "\n",
    "print(masked_labels)\n",
    "print(predicted_labels)\n",
    "print(labels)\n",
    "# Visualize predicted vs true labels\n",
    "plt.subplot(121)\n",
    "plt.imshow(labels.reshape((-1, 10)))\n",
    "plt.subplot(122)\n",
    "plt.imshow(predicted_labels.reshape((-1, 10)))\n",
    "plt.title(\"Accuracy: {}\".format(accuracy))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ssl' has no attribute 'SSLWantReadError'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "params_face_rec['eps'] = 0.8\n",
    "params_face_rec['k'] = 0\n",
    "\n",
    "epss = [10**(-i) for i in range(5)]\n",
    "for eps in epss:\n",
    "    print(f'eps = {eps}')\n",
    "    params_face_rec['eps'] = eps\n",
    "        \n",
    "    # graph Laplacian\n",
    "    L = build_laplacian_regularized(images, \n",
    "                                    params_face_rec['laplacian_regularization'], \n",
    "                                    params_face_rec['var'], \n",
    "                                    params_face_rec['eps'], \n",
    "                                    params_face_rec['k'], \n",
    "                                    params_face_rec['laplacian_normalization'])\n",
    "\n",
    "    # Run HFS\n",
    "    predicted_labels, f = compute_hfs(L, masked_labels, soft=True, **params_face_rec)\n",
    "    accuracy = np.equal(predicted_labels, labels).mean()\n",
    "    print(\"Accuracy = \", accuracy)\n",
    "\n",
    "# Visualize predicted vs true labels\n",
    "plt.subplot(121)\n",
    "plt.imshow(labels.reshape((-1, 10)))\n",
    "plt.subplot(122)\n",
    "plt.imshow(predicted_labels.reshape((-1, 10)))\n",
    "plt.title(\"Accuracy: {}\".format(accuracy))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ssl' has no attribute 'SSLWantReadError'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# graph Laplacian\n",
    "L = build_laplacian_regularized(images_a, \n",
    "                                params_face_rec['laplacian_regularization'], \n",
    "                                params_face_rec['var'], \n",
    "                                params_face_rec['eps'], \n",
    "                                params_face_rec['k'], \n",
    "                                params_face_rec['laplacian_normalization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ssl' has no attribute 'SSLWantReadError'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Run HFS\n",
    "epss = [0.999,0.95,0.9,0.85,0.75,]\n",
    "for eps in epss:\n",
    "    print(f'eps = {eps}')\n",
    "    params_face_rec['eps'] = eps\n",
    "        \n",
    "    L = build_laplacian_regularized(images_a, \n",
    "                                params_face_rec['laplacian_regularization'], \n",
    "                                params_face_rec['var'], \n",
    "                                params_face_rec['eps'], \n",
    "                                params_face_rec['k'], \n",
    "                                params_face_rec['laplacian_normalization'])\n",
    "\n",
    "    predicted_labels_a, f = compute_hfs(L, masked_labels_a, soft=True, **params_face_rec)\n",
    "    accuracy_a = np.equal(predicted_labels_a, labels_a).mean()\n",
    "    print(\"Accuracy = \", accuracy_a)\n",
    "\n",
    "# print(masked_labels_a)\n",
    "# print(predicted_labels_a)\n",
    "# print(labels_a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Online SSL\n",
    "\n",
    "Now, instead of having all the data available at once, images will be received online: at each time $t$, a new image $x_t$ is observed and the algorithm has to output a label $y_t$. \n",
    "\n",
    "Use the function `create_user_profile` to capture a training set of labeled data (of your face and someone else). The faces will be preprocessed and saved in the folder `data/faces`. They will be loaded by `online_face_recognition`.\n",
    "\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Choose the hyperparameters and complete the functions `online_ssl_update_centroids` and `online_ssl_compute_solution`. \n",
    "\n",
    "Modify your code to be able to disregard faces it cannot recognize.\n",
    "\n",
    "* Tips:\n",
    "    * You can use the functions `build_similarity_graph` and `build_laplacian`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1 - Attach to this notebook some of the resulting frames of online face recognition. \n",
    "\n",
    "* Tips: \n",
    "    * You can save the resulting frame and add it to the notebook in a markdown cell as `![title](picture.png)`\n",
    "\n",
    "Result for online face recognition with users Travolta and Samuel L. Jackson:\n",
    "![Result for online face recognition with users Travolta and Samuel L. Jackson](results/frame_samuel_l_jackson.png)\n",
    "![Result for online face recognition with users Travolta and Samuel L. Jackson](results/frame_travolta.png)\n",
    "\n",
    "Result for online face recognition with users Antoine and Mathis:\n",
    "![Result for online face recognition with users Antoine and Mathis](results/frame_antoine_mathis.png)\n",
    "\n",
    "\n",
    "### Question 3.2 - What strategy did you use to label a face as unknown? Attach to this notebook an example of a unknown face being correctly labeled as unknown.\n",
    "\n",
    "* Tips\n",
    "    * If you identify a face as unknown, you can return `[(\"unknown\", score)]` from the function `online_ssl_compute_solution`.\n",
    "\n",
    "I identify a face as unknown if the score is negative, we could use a treshold.\n",
    "\n",
    "Result for online face recognition with only user Mathis:\n",
    "![Result for online face recognition with user Mathis](results/frame_mathis_unknown.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ssl' has no attribute 'SSLWantReadError'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    "import sys\n",
    "from scipy.spatial import distance\n",
    "import scipy.io as sio\n",
    "\n",
    "from helper_online_ssl import create_user_profile, online_face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ssl' has no attribute 'SSLWantReadError'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Define parameters for face recognition with HFS\n",
    "\"\"\"\n",
    "params_online_ssl = {}\n",
    "params_online_ssl['laplacian_regularization'] = 1.0\n",
    "params_online_ssl['var'] = 10_000.0\n",
    "params_online_ssl['eps'] = 0.95\n",
    "params_online_ssl['k'] = 0\n",
    "params_online_ssl['laplacian_normalization'] = 'unn'\n",
    "params_online_ssl['c_l'] = 1\n",
    "params_online_ssl['c_u'] = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ssl' has no attribute 'SSLWantReadError'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class IncrementalKCenters:\n",
    "    def __init__(self, labeled_faces, labels, label_names, max_num_centroids=50):\n",
    "        #  Number of labels\n",
    "        self.n_labels = max(labels)\n",
    "\n",
    "        #  Dimension of the input image\n",
    "        self.image_dimension = labeled_faces.shape[1]\n",
    "\n",
    "        #  Check input validity\n",
    "        assert (set(labels) == set(\n",
    "            range(1, 1 + self.n_labels))), \"Initially provided faces should be labeled in [1, max]\"\n",
    "        assert (len(labeled_faces) == len(labels)), \"Initial faces and initial labels are not of same size\"\n",
    "\n",
    "        #  Number of labelled faces\n",
    "        self.n_labeled_faces = len(labeled_faces)\n",
    "\n",
    "        # Model parameter : number of maximum stored centroids\n",
    "        self.max_num_centroids = max_num_centroids\n",
    "\n",
    "        # Model centroids (inital labeled faces). Shape = (number_of_centroids, dimension)\n",
    "        self.centroids = labeled_faces\n",
    "\n",
    "        # Centroids labels\n",
    "        self.Y = labels\n",
    "        \n",
    "        # Label names (= user names)\n",
    "        self.label_names = label_names\n",
    "\n",
    "        # Variables that are initialized in online_ssl_update_centroids()\n",
    "        self.centroids_distances = None\n",
    "        self.taboo = None\n",
    "        self.V = None\n",
    "        self.init = True\n",
    "\n",
    "        # index of x_t (initialized later)\n",
    "        self.last_face = None\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Initialization after the first time that the maximum number of centroids is reached.\n",
    "        \"\"\"       \n",
    "        #  Compute the centroids distances\n",
    "        self.centroids_distances = distance.cdist(self.centroids, self.centroids)\n",
    "\n",
    "        #  set labeled nodes and self loops as infinitely distant, to avoid merging labeled centroids\n",
    "        np.fill_diagonal(self.centroids_distances, +np.Inf)\n",
    "        self.centroids_distances[0:self.n_labeled_faces, 0:self.n_labeled_faces] = +np.Inf\n",
    "\n",
    "        # put labeled nodes in the taboo list\n",
    "        self.taboo = np.array(range(self.centroids.shape[0])) < self.n_labeled_faces\n",
    "\n",
    "        # initialize multiplicity\n",
    "        self.V = np.ones(self.centroids.shape[0])\n",
    "\n",
    "\n",
    "    def online_ssl_update_centroids(self, face):\n",
    "        \"\"\"\n",
    "        TO BE COMPLETED\n",
    "\n",
    "        Update centroids, multiplicity vector V, labels Y.\n",
    "        \n",
    "        Note: In Y, set label to 0 for unlabeled faces.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        face : array\n",
    "            New sample\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        List with the scores for each possible label:\n",
    "            [(label_1, score_1), (label_2, score_2), ...]\n",
    "        \"\"\"\n",
    "\n",
    "        assert (self.image_dimension == len(face)), \"new image not of good size\"\n",
    "\n",
    "        # Case 1: maximum number of centroids has been reached.\n",
    "        if self.centroids.shape[0] >= self.max_num_centroids + 1:\n",
    "            if self.init:\n",
    "                #  Initialization after the first time that the maximum number of centroids is reached\n",
    "                self.initialize()\n",
    "                self.init = False\n",
    "            \"\"\"\n",
    "            Find c_rep and c_add following Algorithm 1.\n",
    "            \n",
    "            - c_1, c_2 = two closest centroids (minimum distance) such that at least one of them is not in self.taboo.\n",
    "            - c_rep = centroid in {c_1, c_2} that is in self.taboo. If none of them is in self.taboo, c_rep is the one\n",
    "                      with largest multiplicity.\n",
    "            - c_add = centroid in {c_1, c_2} that is not c_rep.\n",
    "            \"\"\"\n",
    "\n",
    "            centroid_distance_to_new_data = distance.cdist([face],self.centroids)\n",
    "            # print(centroid_distance_to_new_data)\n",
    "\n",
    "            indx = np.argsort(centroid_distance_to_new_data)[0]\n",
    "            c_1_idx = indx[0]\n",
    "            c_2_idx = indx[1]\n",
    "            if self.taboo[c_1_idx]:\n",
    "                idx = 1\n",
    "                while self.taboo[c_2_idx]:\n",
    "                    idx += 1\n",
    "                    c_2_idx = indx[idx]\n",
    "\n",
    "            c_1 = self.centroids[c_1_idx]\n",
    "            c_2 = self.centroids[c_2_idx]\n",
    "\n",
    "            v_1 = self.V[c_1_idx]\n",
    "            v_2 = self.V[c_2_idx]\n",
    "            \n",
    "            if v_1 >= v_2:\n",
    "                c_rep, c_add = c_1, c_2\n",
    "                c_rep_idx, c_add_idx = c_1_idx, c_2_idx\n",
    "                v_rep, v_add = v_1, v_2\n",
    "            else:\n",
    "                c_rep, c_add = c_2, c_1\n",
    "                c_rep_idx, c_add_idx = c_2_idx, c_1_idx\n",
    "                v_rep, v_add = v_2, v_1\n",
    "\n",
    "            if self.taboo[c_1_idx]:\n",
    "                c_rep, c_add = c_2, c_1\n",
    "                c_rep_idx, c_add_idx = c_2_idx, c_1_idx\n",
    "                v_rep, v_add = v_2, v_1\n",
    "\n",
    "            if self.taboo[c_2_idx]:\n",
    "                c_rep, c_add = c_1, c_2\n",
    "                c_rep_idx, c_add_idx = c_1_idx, c_2_idx\n",
    "                v_rep, v_add = v_1, v_2\n",
    "\n",
    "            # ...\n",
    "            \n",
    "\n",
    "            \"\"\"\n",
    "            Update data structures: self.centroids and self.V\n",
    "            \"\"\"\n",
    "            # ...\n",
    "            self.V[c_rep_idx] += self.V[c_add_idx]\n",
    "            self.centroids[c_add_idx] = face\n",
    "            c_add = c_add_idx\n",
    "            self.V[c_add_idx] = 1\n",
    "            \n",
    "\n",
    "            \"\"\"\n",
    "            Update the matrix containing the distances.\n",
    "            \"\"\"\n",
    "            dist_row = distance.cdist(np.array([self.centroids[c_add]]), self.centroids)[0]\n",
    "            dist_row[c_add] = +np.inf\n",
    "            self.centroids_distances[c_add, :] = dist_row\n",
    "            self.centroids_distances[:, c_add] = dist_row\n",
    "            self.last_face = c_add\n",
    "\n",
    "        # Case 2: create new centroid with face\n",
    "        # Remark: the multiplicities vector self.V is initialized in self.initialize()\n",
    "        else:\n",
    "            current_len = len(self.centroids)\n",
    "            self.Y = np.append(self.Y, 0)\n",
    "            self.centroids = np.vstack([self.centroids, face])\n",
    "\n",
    "    def online_ssl_compute_solution(self):\n",
    "        \"\"\"\n",
    "        TO BE COMPLETED.\n",
    "\n",
    "        Returns a prediction corresponding to self.last_face.\n",
    "        \"\"\"\n",
    "        # Multiplicity matrix\n",
    "        if self.init:\n",
    "            V = np.diag(np.ones(self.centroids.shape[0]))\n",
    "            self.last_face = self.centroids.shape[0] - 1\n",
    "        else:\n",
    "            V = np.diag(self.V)\n",
    "            \n",
    "        # Build quantized graph and its regularized Laplacian\n",
    "        W = build_similarity_graph(self.centroids,\n",
    "                                     var = params_online_ssl['var'],\n",
    "                                     eps = params_online_ssl['eps'],\n",
    "                                     k = params_online_ssl['k'])\n",
    "        W = V @ W @ V\n",
    "        L = build_laplacian(W,laplacian_normalization=params_online_ssl['laplacian_normalization'])\n",
    "        Q = L + params_online_ssl['laplacian_regularization']  *  V # np.ones(self.centroids.shape[0])\n",
    "        # Q = build_laplacian_regularized(self.centroids, 0.01, 'unn',eps = 0.01)\n",
    "        # Compute the hard HFS solution f. \n",
    "        labels, f = compute_hfs(Q, self.Y, soft=False, **params_online_ssl)\n",
    "\n",
    "        # Return the score for each possible label\n",
    "        num_classes = len(np.unique(self.Y))-1 \n",
    "        label_scores = []\n",
    "        for ii in range(num_classes):\n",
    "            label = self.label_names[ii]\n",
    "            score = f[self.last_face, ii]\n",
    "\n",
    "            treshold  = 0.\n",
    "            if score <= treshold :\n",
    "                label = 'unknown'\n",
    "            label_scores.append((label, score))\n",
    "        \n",
    "        \n",
    "        return label_scores\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ssl' has no attribute 'SSLWantReadError'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "create_user_profile('Travolta',video_filename='data/pulp_fiction.mp4')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ssl' has no attribute 'SSLWantReadError'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "create_user_profile('Samuel L. Jackson',video_filename='data/pulp_fiction.mp4')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ssl' has no attribute 'SSLWantReadError'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "create_user_profile('Antoine')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ssl' has no attribute 'SSLWantReadError'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "create_user_profile('Mathis')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ssl' has no attribute 'SSLWantReadError'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "online_face_recognition(['Mathis'], IncrementalKCenters, n_pictures=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ssl' has no attribute 'SSLWantReadError'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "online_face_recognition(['Travolta', 'Samuel L. Jackson'], IncrementalKCenters, n_pictures=15,video_filename='data/pulp_fiction_low.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mAttributeError: module 'ssl' has no attribute 'SSLWantReadError'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "online_face_recognition(['Antoine', 'Mathis'], IncrementalKCenters, n_pictures=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1 - You can now classify faces as either known (i.e. belong to a class) or unknown (i.e. belong to no class). How would you modify the algorithm to automatically learn to classify new classes? That is, how can you modify the algorithm so that faces that appear unfrequently are labeled as unknown, but once a specific face has been seen enough times it gets assigned an automatic label (e.g. AutoBob) and from that point it is treated as a new class. \n",
    "\n",
    "* Tips: \n",
    "    * Think back to the idea of incremental clustering as unsupervised classification\n",
    "\n",
    "An approach could be the following.\n",
    "As new unknown faces are added to the graph, a can cluster naturally form. When a cluster  accumulates a sufficient number of instances, signaling the presence of a new face, a new label can be introduced and assigned to the nodes within that emergent cluster. \n",
    "\n",
    "    \n",
    "### Question 4.2 - In class we considered different kinds of metric for (semi) supervised learning. Looking at the face classification task, try to quantify how the offline-online, exact-quantized, and inductive-transductive axes influence each other. In particular given $l$ labeled faces, $u = N - l$ unlabeled faces, and $m$ extra/test faces design an experimental study to quantify these trade-offs, both in terms of transductive and inductive error, as well as online/batch error. Examples of combinations that can be used to study these axes are:\n",
    "* Supervised vs Semi-supervised\n",
    "    * A comparison between a supervised learner (of your choice) trained on the $l$ labeled faces and a semi-supervised learner (of your choice) trained on the $l + u$ labeled and unlabeled faces\n",
    "    * A comparison between a supervised learner (of your choice) trained on the $N$ labeled and unlabeled (you reveal everything here) faces and a semi-supervised learner (of your choice) trained on the $l + u$ labeled and unlabeled faces.\n",
    "* Inductive vs Transductive\n",
    "    * A comparison between a supervised learner (of your choice) trained on the $l$ labeled faces and a semi-supervised learner (of your choice) trained on the $l + u$ labeled and unlabeled faces evaluated on the $N$ revealed points and then on the $m$ unrevealed points.\n",
    "* Supervised vs Semi-supervised and Inductive vs Transductive\n",
    "    * A comparison between a supervised learner (of your choice) trained on the $N$ labeled and unlabeled (you reveal everything here) faces and a semi-supervised learner (of your choice) trained on the $l + u$ labeled and unlabeled faces, evaluated on the $N$ revealed points and then on the $m$ unrevealed points.\n",
    "* Online vs Batch\n",
    "    * A comparison between a supervised learner (of your choice) trained on the $N$ labeled faces and an online supervised learner (of your choice) trained revealing the $N$ labels one at a time\n",
    "* Exact vs Quantized\n",
    "    * A comparison between a semi-supervised learner (of your choice) trained on the $N$ labeled and unlabeled (you reveal everything here) faces using a certain memory budget, and the same learner with a constrained memory budget.\n",
    "* Exact vs Quantized and Online vs Batch and Inductive vs Transductive\n",
    "    * A comparison between a semi-supervised online learner (of your choice) trained with and without quantization, evaluated both on the $N$ faces revealed during training, and $m$ faces unrevealed. You can further compare the online performance of the learner against the performance of an \"hindsight\" learner that saw the labels all at once\n",
    "\n",
    "I'm already late."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
