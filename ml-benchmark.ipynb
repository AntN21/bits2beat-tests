{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark for signal representation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multifractal analysis\n",
    "\n",
    "- Discrete Fourier Transform (DFT)\n",
    "- Spectrogram\n",
    "- Local symbolic features\n",
    "- SAX representation\n",
    "- Approximate entropy\n",
    "\n",
    "ML\n",
    "\n",
    "- Autoencoder\n",
    "\n",
    "- RNN\n",
    "- LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pywt\n",
    "\n",
    "import orjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymultifracs.mfa as mfa\n",
    "from pymultifracs.utils import build_q_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# import pandas_datareader as pdr\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.api import acf, graphics, pacf\n",
    "from statsmodels.tsa.ar_model import AutoReg, ar_select_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian process covariance function\n",
    "def covariance_function(x):\n",
    "    return 1. * np.exp(-((x / 10.)**2))\n",
    "\n",
    "# Gaussian process simulation function\n",
    "def sample_gaussian_process(c):\n",
    "    \"\"\"\n",
    "    c array representing covariance function computed in x\n",
    "    \"\"\"\n",
    "    white_noise = np.random.normal(0, 1, len(c))\n",
    "\n",
    "    fc = np.fft.fft(np.fft.ifftshift(c))\n",
    "    fw = np.fft.fft(white_noise)\n",
    "    fz = np.sqrt(fc) * fw\n",
    "\n",
    "    return np.fft.fftshift(np.fft.ifft(fz)).real\n",
    "\n",
    "c = covariance_function(np.arange(0,100,1))\n",
    "\n",
    "X = torch.FloatTensor( np.array([sample_gaussian_process(c) for _ in range(50)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformator():\n",
    "    def __init__(self) -> None:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "class DataTransform():\n",
    "    def __init__(self) -> None:\n",
    "        self.transformed_X = None\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def X(self):\n",
    "        return self.transformed_X\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.transformed_X.shape\n",
    "    \n",
    "    # @staticmethod\n",
    "    def identity(self,X):\n",
    "        return torch.FloatTensor(X)\n",
    "    \n",
    "    # @staticmethod\n",
    "    def fourier(self,X, new_dimentsion = None):\n",
    "        # Compute the Fourier transform\n",
    "        fourier_transform = torch.fft.fft(X,n=new_dimentsion)\n",
    "        \n",
    "        # Compute the modulus (magnitude) of the Fourier transform\n",
    "        modulus = torch.abs(fourier_transform)\n",
    "        \n",
    "        return modulus\n",
    "    \n",
    "    # @staticmethod\n",
    "    def wavedec(self,X, wavelet='db1', mode='symmetric'):\n",
    "        \n",
    "        # Convert PyTorch tensor to NumPy array\n",
    "        array = X.numpy()\n",
    "    \n",
    "        # Compute the wavelet transform\n",
    "        coeffs = pywt.wavedec(array, wavelet, mode=mode)\n",
    "\n",
    "        # coeffs_array = pywt.coeffs_to_array(coeffs)\n",
    "        # # Convert the result back to PyTorch tensors\n",
    "        coeffs_torch = [torch.tensor(c) for c in coeffs]\n",
    "       \n",
    "    \n",
    "        return coeffs_torch\n",
    "    \n",
    "    def wavedec(self,X, wavelet='db1', mode='symmetric'):\n",
    "        \n",
    "        # Convert PyTorch tensor to NumPy array\n",
    "        array = X.numpy()\n",
    "    \n",
    "        # Compute the wavelet transform\n",
    "        coeffs = pywt.wavedec(array, wavelet, mode=mode)\n",
    "\n",
    "        coeffs_array = pywt.coeffs_to_array(coeffs)\n",
    "        # # Convert the result back to PyTorch tensors\n",
    "        # coeffs_torch = [torch.tensor(c) for c in coeffs]\n",
    "        # # coeffs_torch = torch.FloatTensor(coeffs_array)\n",
    "        # return torch.cat(coeffs_torch,dim=-1)\n",
    "        coeffs_array = pywt.coeffs_to_array(coeffs)\n",
    "    \n",
    "        return torch.FloatTensor(coeffs_array[0])\n",
    "    \n",
    "    def wavedec(self,X,level=4, wavelet='db1', mode='symmetric'):\n",
    "        \n",
    "        # Convert PyTorch tensor to NumPy array\n",
    "        array = X.numpy()\n",
    "    \n",
    "        # Compute the wavelet transform\n",
    "        coeffs = pywt.wavedec(array, wavelet, mode=mode,level=level)\n",
    "\n",
    "        # coeffs_array = pywt.coeffs_to_array(coeffs)\n",
    "        # # Convert the result back to PyTorch tensors\n",
    "        coeffs_torch = [torch.tensor(c) for c in coeffs[:1]]\n",
    "       \n",
    "        return torch.cat(coeffs_torch,dim=-1)\n",
    "    # @staticmethod\n",
    "    def dwt(self,X, wavelet='db1', mode='symmetric'):\n",
    "        # Convert PyTorch tensor to NumPy array\n",
    "        array = X.numpy()\n",
    "    \n",
    "        # Compute the wavelet transform\n",
    "        coeffs = pywt.dwt(array, wavelet, mode=mode)\n",
    "    \n",
    "        # Convert the result back to PyTorch tensors\n",
    "        coeffs_torch = [torch.tensor(c) for c in coeffs]\n",
    "    \n",
    "        return torch.cat(coeffs_torch,dim=-1)\n",
    "    \n",
    "    # @staticmethod\n",
    "    def get_ar_coefficients(self, X, k):\n",
    "        n, p = X.shape\n",
    "        ar_coefficients = np.zeros((n, k))\n",
    "\n",
    "        for i in range(n):\n",
    "            # Fit the AR model to the i-th time series\n",
    "            model = AutoReg(X[i], lags=k).fit()\n",
    "            \n",
    "            # Extract the coefficients (excluding the intercept term if present)\n",
    "            ar_coefficients[i] = model.params[1:k+1]  # assuming the intercept is model.params[0]\n",
    "\n",
    "        return ar_coefficients\n",
    "    \n",
    "    # @staticmethod\n",
    "    def autoreg(self,X,k):\n",
    "        return torch.tensor(self.get_ar_coefficients(X,k))\n",
    "    \n",
    "    def shannon_encoding(self,X,level,wavelet='db1',mode='symmetric'):\n",
    "\n",
    "        def compute_shannon_entropy(signal):\n",
    "            \"\"\"Here, we will compute nonnormalized shannon entropy.\n",
    "            We will basically implement MATLAB function 'wentropy'.\n",
    "            Input: signal should be 1D numpy array.\"\"\"\n",
    "            return -np.nansum(signal**2 * np.log(signal**2)) # nansum to exclude pesky terms like \"0*np.log(0)\"\n",
    "        \n",
    "\n",
    "        n_examples = X.shape[0]\n",
    "        wp = pywt.WaveletPacket(X[0,:], wavelet = \"sym8\", maxlevel = 3)\n",
    "        packet_names = [node.path for node in wp.get_level(3, \"natural\")]  # Packet node names. \n",
    "        \n",
    "        feature_matrix_wav_packet_entropy = np.repeat(np.nan, n_examples*8).reshape(n_examples,8)\n",
    "        for i in range(len(X)):\n",
    "            wp = pywt.WaveletPacket(X[i,:], wavelet = \"sym8\", maxlevel = 3) # Wavelet packet transformation\n",
    "            for j in range(len(packet_names)):\n",
    "                new_wp = pywt.WaveletPacket(data = None, wavelet = \"sym8\", maxlevel = 3)\n",
    "                new_wp[packet_names[j]] = wp[packet_names[j]].data\n",
    "                reconstructed_signal = new_wp.reconstruct(update = False) # Signal reconstruction from wavelet packet coefficients\n",
    "                feature_matrix_wav_packet_entropy[i,j] = compute_shannon_entropy(reconstructed_signal) # Entropy of reconstructed signal for every node\n",
    "                # print(reconstructed_signal)\n",
    "                # print(feature_matrix_wav_packet_entropy[i,j])\n",
    "        return feature_matrix_wav_packet_entropy\n",
    "    \n",
    "    def wavelet_leaders(self,X,j1=2,j2=6):\n",
    "        n = X.shape[0] if X.ndim > 1 else 1\n",
    "        transformed_X = -  np.ones((n,2))\n",
    "        for i in range(X.shape[0]):\n",
    "            dwt, lwt = mfa.mf_analysis_full(X[i],\n",
    "                scaling_ranges=[(2, 6)],\n",
    "                q=build_q_log(1, 10, 20),\n",
    "                n_cumul=2,\n",
    "                p_exp=np.inf,\n",
    "                gamint=0.0\n",
    "            )\n",
    "            sf, cumul, mfs, hmin = lwt\n",
    "            transformed_X[i,:] = sf.H.item(), cumul.log_cumulants[1].item()\n",
    "\n",
    "        return transformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransform:\n",
    "    def __init__(self) -> None:\n",
    "        self.transformed_X = None\n",
    "\n",
    "    @property\n",
    "    def X(self):\n",
    "        return self.transformed_X\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        if self.transformed_X is not None:\n",
    "            return self.transformed_X.shape\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def identity(X):\n",
    "        return torch.FloatTensor(X)\n",
    "    \n",
    "    @staticmethod\n",
    "    def fourier(X, new_dimension=None):\n",
    "        fourier_transform = torch.fft.fft(X, n=new_dimension)\n",
    "        modulus = torch.abs(fourier_transform)\n",
    "        return modulus\n",
    "    \n",
    "    @staticmethod\n",
    "    def wavedec(X, level=4, wavelet='db1', mode='symmetric'):\n",
    "        array = X.numpy()\n",
    "        coeffs = pywt.wavedec(array, wavelet, mode=mode, level=level)\n",
    "        coeffs_torch = [torch.tensor(c) for c in coeffs[:1]]\n",
    "        return torch.cat(coeffs_torch, dim=-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def dwt(X, wavelet='db1', mode='symmetric'):\n",
    "        array = X.numpy()\n",
    "        coeffs = pywt.dwt(array, wavelet, mode=mode)\n",
    "        coeffs_torch = [torch.tensor(c) for c in coeffs]\n",
    "        return torch.cat(coeffs_torch, dim=-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_ar_coefficients(X, k):\n",
    "        n, p = X.shape\n",
    "        ar_coefficients = np.zeros((n, k))\n",
    "        for i in range(n):\n",
    "            model = AutoReg(X[i], lags=k).fit()\n",
    "            ar_coefficients[i] = model.params[1:k+1] \n",
    "        return ar_coefficients\n",
    "    \n",
    "    @staticmethod\n",
    "    def autoreg(X, k):\n",
    "        return torch.tensor(DataTransform.get_ar_coefficients(X, k))\n",
    "    \n",
    "    @staticmethod\n",
    "    def shannon_encoding(X, level=4, wavelet='db1', mode='symmetric'):\n",
    "        def compute_shannon_entropy(signal):\n",
    "            return -np.nansum(signal**2 * np.log(signal**2))\n",
    "        \n",
    "        n_examples = X.shape[0]\n",
    "        wp = pywt.WaveletPacket(X[0, :], wavelet=\"sym8\", maxlevel=3)\n",
    "        packet_names = [node.path for node in wp.get_level(3, \"natural\")]\n",
    "        \n",
    "        feature_matrix_wav_packet_entropy = np.full((n_examples, 8), np.nan)\n",
    "        for i in range(len(X)):\n",
    "            wp = pywt.WaveletPacket(X[i, :], wavelet=\"sym8\", maxlevel=3)\n",
    "            for j in range(len(packet_names)):\n",
    "                new_wp = pywt.WaveletPacket(data=None, wavelet=\"sym8\", maxlevel=3)\n",
    "                new_wp[packet_names[j]] = wp[packet_names[j]].data\n",
    "                reconstructed_signal = new_wp.reconstruct(update=False)\n",
    "                feature_matrix_wav_packet_entropy[i, j] = compute_shannon_entropy(reconstructed_signal)\n",
    "        return feature_matrix_wav_packet_entropy\n",
    "    \n",
    "    @staticmethod\n",
    "    def wavelet_leaders(X, j1=2, j2=6):\n",
    "        n = X.shape[0] if X.ndim > 1 else 1\n",
    "        transformed_X = -np.ones((n, 2))\n",
    "        for i in range(X.shape[0]):\n",
    "            dwt, lwt = mfa.mf_analysis_full(\n",
    "                X[i],\n",
    "                scaling_ranges=[(j1, j2)],\n",
    "                q=mfa.build_q_log(1, 10, 20),\n",
    "                n_cumul=2,\n",
    "                p_exp=np.inf,\n",
    "                gamint=0.0\n",
    "            )\n",
    "            sf, cumul, mfs, hmin = lwt\n",
    "            transformed_X[i, :] = sf.H.item(), cumul.log_cumulants[1].item()\n",
    "        return transformed_X\n",
    "\n",
    "    def apply_transformation(self, X, transformation_name, **kwargs):\n",
    "        transformation_methods = {\n",
    "            'identity': self.identity,\n",
    "            'fourier': self.fourier,\n",
    "            'wavedec': self.wavedec,\n",
    "            'dwt': self.dwt,\n",
    "            'autoreg': self.autoreg,\n",
    "            'shannon_encoding': self.shannon_encoding,\n",
    "            'wavelet_leaders': self.wavelet_leaders,\n",
    "        }\n",
    "        \n",
    "        if transformation_name in transformation_methods:\n",
    "            return transformation_methods[transformation_name](X, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Transformation {transformation_name} not recognized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 100])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d,e,f,g = pywt.wavedec(X.numpy(),wavelet='db1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pywt.dwt(X.numpy(),wavelet='db1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1.8800, 0.9046]), tensor([-0.2584, -1.5160]), tensor([ 0.2779,  5.3936, -3.4864,  0.0000]), tensor([-0.9118,  0.2720,  1.3084, -3.2664,  2.6224, -0.6204,  0.0000]), tensor([-0.6910, -0.0629,  0.5940,  1.0400, -0.9947,  4.4571, -0.0081, -1.3618,\n",
      "         1.3416, -2.0219, -1.1804,  1.4271,  0.0000]), tensor([-0.2979, -0.0144,  1.2894, -0.7946,  2.3672, -0.6655,  1.4477,  0.0663,\n",
      "        -0.8370, -0.4419,  1.8736,  0.3160,  0.1559, -0.7624,  0.1843, -0.8321,\n",
      "         2.4465,  2.3645, -0.6316, -0.3934, -1.5510, -1.4870,  0.6557,  0.3796,\n",
      "        -0.7889]), tensor([-0.3484,  0.3363,  0.2300,  0.0551, -0.2527,  0.3295, -0.1840, -1.2970,\n",
      "        -0.1129,  0.6218, -0.8449,  0.0310,  0.0173,  1.0361,  0.4858, -0.5414,\n",
      "        -0.4050,  0.0672,  0.5786, -1.8602,  0.6107,  0.9640,  0.3565, -0.2018,\n",
      "         0.5241, -0.6477,  0.8863,  0.6659, -0.0942,  0.4226, -0.3055,  0.3139,\n",
      "        -0.8859,  0.3828,  0.6621,  0.9664, -0.5670, -0.1343,  0.6387, -0.1333,\n",
      "         0.4540,  0.3731, -0.5347,  0.3489,  0.5770,  0.3529, -0.5912,  2.0847,\n",
      "        -0.4779,  1.1261])]\n",
      "Coefficient 0: tensor([1.8800, 0.9046])\n",
      "Coefficient 1: tensor([-0.2584, -1.5160])\n",
      "Coefficient 2: tensor([ 0.2779,  5.3936, -3.4864,  0.0000])\n",
      "Coefficient 3: tensor([-0.9118,  0.2720,  1.3084, -3.2664,  2.6224, -0.6204,  0.0000])\n",
      "Coefficient 4: tensor([-0.6910, -0.0629,  0.5940,  1.0400, -0.9947,  4.4571, -0.0081, -1.3618,\n",
      "         1.3416, -2.0219, -1.1804,  1.4271,  0.0000])\n",
      "Coefficient 5: tensor([-0.2979, -0.0144,  1.2894, -0.7946,  2.3672, -0.6655,  1.4477,  0.0663,\n",
      "        -0.8370, -0.4419,  1.8736,  0.3160,  0.1559, -0.7624,  0.1843, -0.8321,\n",
      "         2.4465,  2.3645, -0.6316, -0.3934, -1.5510, -1.4870,  0.6557,  0.3796,\n",
      "        -0.7889])\n",
      "Coefficient 6: tensor([-0.3484,  0.3363,  0.2300,  0.0551, -0.2527,  0.3295, -0.1840, -1.2970,\n",
      "        -0.1129,  0.6218, -0.8449,  0.0310,  0.0173,  1.0361,  0.4858, -0.5414,\n",
      "        -0.4050,  0.0672,  0.5786, -1.8602,  0.6107,  0.9640,  0.3565, -0.2018,\n",
      "         0.5241, -0.6477,  0.8863,  0.6659, -0.0942,  0.4226, -0.3055,  0.3139,\n",
      "        -0.8859,  0.3828,  0.6621,  0.9664, -0.5670, -0.1343,  0.6387, -0.1333,\n",
      "         0.4540,  0.3731, -0.5347,  0.3489,  0.5770,  0.3529, -0.5912,  2.0847,\n",
      "        -0.4779,  1.1261])\n",
      "tensor([ 5.0000, -2.0000, -0.7071, -0.7071])\n",
      "Coefficient 0: 4.999999523162842\n",
      "Coefficient 1: -1.9999998807907104\n",
      "Coefficient 2: -0.7071067690849304\n",
      "Coefficient 3: -0.7071068286895752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aejog\\AppData\\Local\\Temp\\ipykernel_11836\\206756173.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor = torch.tensor(X[0])\n"
     ]
    }
   ],
   "source": [
    "def wavelet_transform(tensor, wavelet='db1', mode='symmetric'):\n",
    "    # Convert PyTorch tensor to NumPy array\n",
    "    array = tensor.numpy()\n",
    "    \n",
    "    # Compute the wavelet transform\n",
    "    coeffs = pywt.wavedec(array, wavelet, mode=mode)\n",
    "    \n",
    "    # Convert the result back to PyTorch tensors\n",
    "    coeffs_torch = [torch.tensor(c) for c in coeffs]\n",
    "    # coeffs_array = pywt.coeffs_to_array(coeffs)\n",
    "    # # Convert the result back to PyTorch tensors\n",
    "    # coeffs_torch = [torch.tensor(c) for c in coeffs]\n",
    "    # # coeffs_torch = torch.FloatTensor(coeffs_array)\n",
    "    \n",
    "    return coeffs_torch\n",
    "\n",
    "def wavelet_transform2(tensor, wavelet='db1', mode='symmetric'):\n",
    "    # Convert PyTorch tensor to NumPy array\n",
    "    array = tensor.numpy()\n",
    "    \n",
    "    # Compute the wavelet transform\n",
    "    coeffs = pywt.wavedec(array, wavelet, mode=mode)\n",
    "    \n",
    "    coeffs_array = pywt.coeffs_to_array(coeffs)\n",
    "    \n",
    "    return torch.FloatTensor(coeffs_array[0])\n",
    "# Example usage:\n",
    "tensor = torch.tensor(X[0])\n",
    "wavelet_coeffs = wavelet_transform(tensor)\n",
    "print(wavelet_coeffs)\n",
    "for i, coeff in enumerate(wavelet_coeffs):\n",
    "    print(f'Coefficient {i}: {coeff}')\n",
    "\n",
    "tensor = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "wavelet_coeffs = wavelet_transform2(tensor)\n",
    "print(wavelet_coeffs)\n",
    "for i, coeff in enumerate(wavelet_coeffs):\n",
    "    print(f'Coefficient {i}: {coeff}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bior1.1',\n",
       " 'bior1.3',\n",
       " 'bior1.5',\n",
       " 'bior2.2',\n",
       " 'bior2.4',\n",
       " 'bior2.6',\n",
       " 'bior2.8',\n",
       " 'bior3.1',\n",
       " 'bior3.3',\n",
       " 'bior3.5',\n",
       " 'bior3.7',\n",
       " 'bior3.9',\n",
       " 'bior4.4',\n",
       " 'bior5.5',\n",
       " 'bior6.8',\n",
       " 'cgau1',\n",
       " 'cgau2',\n",
       " 'cgau3',\n",
       " 'cgau4',\n",
       " 'cgau5',\n",
       " 'cgau6',\n",
       " 'cgau7',\n",
       " 'cgau8',\n",
       " 'cmor',\n",
       " 'coif1',\n",
       " 'coif2',\n",
       " 'coif3',\n",
       " 'coif4',\n",
       " 'coif5',\n",
       " 'coif6',\n",
       " 'coif7',\n",
       " 'coif8',\n",
       " 'coif9',\n",
       " 'coif10',\n",
       " 'coif11',\n",
       " 'coif12',\n",
       " 'coif13',\n",
       " 'coif14',\n",
       " 'coif15',\n",
       " 'coif16',\n",
       " 'coif17',\n",
       " 'db1',\n",
       " 'db2',\n",
       " 'db3',\n",
       " 'db4',\n",
       " 'db5',\n",
       " 'db6',\n",
       " 'db7',\n",
       " 'db8',\n",
       " 'db9',\n",
       " 'db10',\n",
       " 'db11',\n",
       " 'db12',\n",
       " 'db13',\n",
       " 'db14',\n",
       " 'db15',\n",
       " 'db16',\n",
       " 'db17',\n",
       " 'db18',\n",
       " 'db19',\n",
       " 'db20',\n",
       " 'db21',\n",
       " 'db22',\n",
       " 'db23',\n",
       " 'db24',\n",
       " 'db25',\n",
       " 'db26',\n",
       " 'db27',\n",
       " 'db28',\n",
       " 'db29',\n",
       " 'db30',\n",
       " 'db31',\n",
       " 'db32',\n",
       " 'db33',\n",
       " 'db34',\n",
       " 'db35',\n",
       " 'db36',\n",
       " 'db37',\n",
       " 'db38',\n",
       " 'dmey',\n",
       " 'fbsp',\n",
       " 'gaus1',\n",
       " 'gaus2',\n",
       " 'gaus3',\n",
       " 'gaus4',\n",
       " 'gaus5',\n",
       " 'gaus6',\n",
       " 'gaus7',\n",
       " 'gaus8',\n",
       " 'haar',\n",
       " 'mexh',\n",
       " 'morl',\n",
       " 'rbio1.1',\n",
       " 'rbio1.3',\n",
       " 'rbio1.5',\n",
       " 'rbio2.2',\n",
       " 'rbio2.4',\n",
       " 'rbio2.6',\n",
       " 'rbio2.8',\n",
       " 'rbio3.1',\n",
       " 'rbio3.3',\n",
       " 'rbio3.5',\n",
       " 'rbio3.7',\n",
       " 'rbio3.9',\n",
       " 'rbio4.4',\n",
       " 'rbio5.5',\n",
       " 'rbio6.8',\n",
       " 'shan',\n",
       " 'sym2',\n",
       " 'sym3',\n",
       " 'sym4',\n",
       " 'sym5',\n",
       " 'sym6',\n",
       " 'sym7',\n",
       " 'sym8',\n",
       " 'sym9',\n",
       " 'sym10',\n",
       " 'sym11',\n",
       " 'sym12',\n",
       " 'sym13',\n",
       " 'sym14',\n",
       " 'sym15',\n",
       " 'sym16',\n",
       " 'sym17',\n",
       " 'sym18',\n",
       " 'sym19',\n",
       " 'sym20']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pywt.wavelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(DataTransform):\n",
    "    def __init__(self, X) -> None:\n",
    "        super().__init__()\n",
    "        self.transformed_X = torch.FloatTensor(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read JSON data using orjson\n",
    "# with open('mydata-mitbih.json', 'r') as f:\n",
    "#     data = orjson.loads(f.read())\n",
    "\n",
    "# # Convert the data to a pandas DataFrame\n",
    "# df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df= pd.read_json('mydata-mitbih-small.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.array(X_df.sample(frac=1))\n",
    "Y_data = X_data[:,-1]\n",
    "X_data = X_data[:,:-1]\n",
    "\n",
    "Y_data = 1.*(Y_data > 0.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_data\n",
    "y = Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict_from_hdf5(filename):\n",
    "    \"\"\"\n",
    "    Load a dictionary of lists of arrays from an HDF5 file.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The name of the HDF5 file to load the data from.\n",
    "\n",
    "    Returns:\n",
    "    dict: The dictionary containing lists of arrays.\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    with h5py.File(filename, 'r') as hdf5_file:\n",
    "        for key in hdf5_file.keys():\n",
    "            group = hdf5_file[key]\n",
    "            data_dict[key] = np.array([group[f'array_{i}'][...] for i in range(len(group))])\n",
    "    return data_dict\n",
    "\n",
    "data_dict = load_dict_from_hdf5('final_data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bidmc-cong', 'mit-bih-ar', 'mit-bih-no'])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "target = 0\n",
    "skip = True\n",
    "for key,value in data_dict.items():\n",
    "    # if  skip:\n",
    "    #     skip  = False\n",
    "    #     continue\n",
    "    n = value.shape[0]\n",
    "    X.append(torch.tensor(np.array(value)))\n",
    "    y.append(torch.ones(n,dtype=int)*target)\n",
    "    target+=1\n",
    "\n",
    "X = torch.cat(X).squeeze()\n",
    "y = torch.cat(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = DataTransform().(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([840, 65032])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "array = np.array(X)\n",
    "\n",
    "# Compute the wavelet transform\n",
    "coeffs = pywt.wavedec(array, 'db2', mode='symmetric')\n",
    "# print(coeffs)\n",
    "# coeffs_array = pywt.coeffs_to_array(coeffs)\n",
    "# # Convert the result back to PyTorch tensors\n",
    "coeffs_torch = [torch.tensor(c) for c in coeffs]\n",
    "#coeffs_torch = torch.FloatTensor(coeffs_array)\n",
    "torch.cat(coeffs_torch,dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([840, 250])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataTransform().fourier(X,250).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modwt import modwt, modwtmra\n",
    "import pandas as pd\n",
    "\n",
    "# dont #work\n",
    "# wt = modwt(np.array(X), 'db2', 5)\n",
    "# wtmra = modwtmra(wt, 'db2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(dwt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pymultifracs.mfa as mfa\n",
    "# from pymultifracs.utils import build_q_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwt, lwt = mfa.mf_analysis_full(X[10],\n",
    "    scaling_ranges = [(2,6)],\n",
    "    q = build_q_log(1, 10, 20),\n",
    "    n_cumul=2,\n",
    "    p_exp=np.inf,\n",
    "    gamint=0.0\n",
    ")\n",
    "lwt_sf, lwt_cumul, lwt_mfs, hmin = lwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X = np.array(X)\n",
    "# n = X.shape[0] if X.ndim > 1 else 1\n",
    "# transformed_X = -  np.ones((n,2))\n",
    "# for i in range(X.shape[0]):\n",
    "#     dwt, lwt = mfa.mf_analysis_full(X[i],\n",
    "#     scaling_ranges = [(2,6)],\n",
    "#     q = build_q_log(1, 10, 20),\n",
    "#     n_cumul=2,\n",
    "#     p_exp=np.inf,\n",
    "#     gamint=0.0\n",
    "# )\n",
    "#     sf, cumul, mfs, hmin = lwt\n",
    "#     transformed_X[i,:] = sf.H.item(), cumul.log_cumulants[1].item()\n",
    "\n",
    "# transformed_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65000])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dwt[])\n",
    "# DataTransform().wavelet_leaders(X[0:100],2,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.fft.fft()\n",
    "# DataTransform().wavelet_leaders(X[0:100],2,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = DataTransform().fourier(X,250)\n",
    "# X = DataTransform().wavedec(X,wavelet='db2')\n",
    "X = DataTransform().shannon_encoding(np.array(X),level=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(840, 8)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "# Generate a sample time series data\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "time_series = np.sin(np.linspace(0, 20, n)) + np.random.normal(scale=0.5, size=n)\n",
    "\n",
    "# Plot the time series\n",
    "plt.plot(time_series)\n",
    "plt.title('Sample Time Series Data')\n",
    "plt.show()\n",
    "\n",
    "# Fit an autoregressive model to the time series data\n",
    "# Using lag value of 4 for demonstration\n",
    "lag = 4\n",
    "model = AutoReg(time_series, lags=lag).fit()\n",
    "\n",
    "# Print the coefficients\n",
    "print(\"Coefficients of the AR model:\")\n",
    "print(model.params)\n",
    "\n",
    "# Predicting future values\n",
    "pred = model.predict(start=len(time_series), end=len(time_series)+10)\n",
    "print(\"\\nPredicted values:\")\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing = np.array(X)\n",
    "# mod = AutoReg(housing[0], 3)\n",
    "# res = mod.fit()\n",
    "# print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aejog\\AppData\\Local\\Temp\\ipykernel_11836\\4202145339.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
      "C:\\Users\\aejog\\AppData\\Local\\Temp\\ipykernel_11836\\4202145339.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "# iris = datasets.load_iris()\n",
    "# X, y = iris.data, iris.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCNN Accuracy: 1.0000\n",
      "CNN Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Define the classifiers\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 50)\n",
    "        self.fc2 = nn.Linear(50, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=2, stride=1)\n",
    "        self.fc1 = nn.Linear(16 * (input_dim - 1), output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_dim, 50, batch_first=True)\n",
    "        self.fc1 = nn.Linear(50, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc1(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "# Function to train and evaluate the classifiers\n",
    "def train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, num_epochs=20):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred.extend(predicted.numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "# Initialize models, criteria, and optimizers\n",
    "input_dim = X.shape[1]\n",
    "output_dim = len(set(y))\n",
    "\n",
    "models = {\n",
    "    'FCNN': FCNN(input_dim, output_dim),\n",
    "    'CNN': SimpleCNN(input_dim, output_dim),\n",
    "    # 'RNN': SimpleRNN(input_dim, output_dim)\n",
    "}\n",
    "\n",
    "criteria = {\n",
    "    'FCNN': nn.CrossEntropyLoss(),\n",
    "    'CNN': nn.CrossEntropyLoss(),\n",
    "    # 'RNN': nn.CrossEntropyLoss()\n",
    "}\n",
    "\n",
    "optimizers = {\n",
    "    'FCNN': optim.Adam(models['FCNN'].parameters(), lr=0.01),\n",
    "    'CNN': optim.Adam(models['CNN'].parameters(), lr=0.01),\n",
    "    # 'RNN': optim.Adam(models['RNN'].parameters(), lr=0.01)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    criterion = criteria[name]\n",
    "    optimizer = optimizers[name]\n",
    "    accuracy = train_and_evaluate(model, train_loader, test_loader, criterion, optimizer)\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wtmra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = \n",
    "# y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# iris = datasets.load_iris()\n",
    "# X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[281:], y[281:] - 1, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "  Accuracy: 1.0000\n",
      "  Cross-Validation Score: 0.9976\n",
      "\n",
      "Random Forest:\n",
      "  Accuracy: 1.0000\n",
      "  Cross-Validation Score: 1.0000\n",
      "\n",
      "SVM:\n",
      "  Accuracy: 1.0000\n",
      "  Cross-Validation Score: 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    cross_val = cross_val_score(clf, X, y, cv=5)\n",
    "    results[name] = {\"Accuracy\": accuracy, \"Cross-Validation Score\": cross_val.mean()}\n",
    "\n",
    "# Print results\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"  Cross-Validation Score: {metrics['Cross-Validation Score']:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[147], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stop\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN:\n",
      "  Accuracy: 0.8452\n",
      "  Cross-Validation Score: 0.8262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "clf = SVC()\n",
    "results = {}\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "cross_val = cross_val_score(clf, X, y, cv=5)\n",
    "results[name] = {\"Accuracy\": accuracy, \"Cross-Validation Score\": cross_val.mean()}\n",
    "\n",
    "# Print results\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"  Cross-Validation Score: {metrics['Cross-Validation Score']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[150], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stop\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     29\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n\u001b[1;32m---> 30\u001b[0m     cross_val \u001b[38;5;241m=\u001b[39m cross_val_score(clf, X, y, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     31\u001b[0m     results[name] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: accuracy, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCross-Validation Score\u001b[39m\u001b[38;5;124m\"\u001b[39m: cross_val\u001b[38;5;241m.\u001b[39mmean()}\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:562\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    560\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 562\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[0;32m    563\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    564\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    565\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    566\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    567\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[0;32m    568\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[0;32m    569\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    570\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    571\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[0;32m    572\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[0;32m    573\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    574\u001b[0m )\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:309\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 309\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    310\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    311\u001b[0m         clone(estimator),\n\u001b[0;32m    312\u001b[0m         X,\n\u001b[0;32m    313\u001b[0m         y,\n\u001b[0;32m    314\u001b[0m         scorers,\n\u001b[0;32m    315\u001b[0m         train,\n\u001b[0;32m    316\u001b[0m         test,\n\u001b[0;32m    317\u001b[0m         verbose,\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    319\u001b[0m         fit_params,\n\u001b[0;32m    320\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[0;32m    321\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    322\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[0;32m    323\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    324\u001b[0m     )\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[0;32m    326\u001b[0m )\n\u001b[0;32m    328\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:732\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    730\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 732\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    736\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:525\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    524\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 525\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stages(\n\u001b[0;32m    526\u001b[0m     X,\n\u001b[0;32m    527\u001b[0m     y,\n\u001b[0;32m    528\u001b[0m     raw_predictions,\n\u001b[0;32m    529\u001b[0m     sample_weight,\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng,\n\u001b[0;32m    531\u001b[0m     X_val,\n\u001b[0;32m    532\u001b[0m     y_val,\n\u001b[0;32m    533\u001b[0m     sample_weight_val,\n\u001b[0;32m    534\u001b[0m     begin_at_stage,\n\u001b[0;32m    535\u001b[0m     monitor,\n\u001b[0;32m    536\u001b[0m )\n\u001b[0;32m    538\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:603\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    596\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m loss_(\n\u001b[0;32m    597\u001b[0m             y[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    598\u001b[0m             raw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    599\u001b[0m             sample_weight[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    600\u001b[0m         )\n\u001b[0;32m    602\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 603\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stage(\n\u001b[0;32m    604\u001b[0m     i,\n\u001b[0;32m    605\u001b[0m     X,\n\u001b[0;32m    606\u001b[0m     y,\n\u001b[0;32m    607\u001b[0m     raw_predictions,\n\u001b[0;32m    608\u001b[0m     sample_weight,\n\u001b[0;32m    609\u001b[0m     sample_mask,\n\u001b[0;32m    610\u001b[0m     random_state,\n\u001b[0;32m    611\u001b[0m     X_csc,\n\u001b[0;32m    612\u001b[0m     X_csr,\n\u001b[0;32m    613\u001b[0m )\n\u001b[0;32m    615\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:245\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    242\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    244\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 245\u001b[0m tree\u001b[38;5;241m.\u001b[39mfit(X, residual, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    248\u001b[0m loss\u001b[38;5;241m.\u001b[39mupdate_terminal_regions(\n\u001b[0;32m    249\u001b[0m     tree\u001b[38;5;241m.\u001b[39mtree_,\n\u001b[0;32m    250\u001b[0m     X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    257\u001b[0m     k\u001b[38;5;241m=\u001b[39mk,\n\u001b[0;32m    258\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m   1321\u001b[0m         X,\n\u001b[0;32m   1322\u001b[0m         y,\n\u001b[0;32m   1323\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1324\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[0;32m   1325\u001b[0m     )\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    # \"Logistic Regression\": LogisticRegression(max_iter=1),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"k-NN\": KNeighborsClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    cross_val = cross_val_score(clf, X, y, cv=5)\n",
    "    results[name] = {\"Accuracy\": accuracy, \"Cross-Validation Score\": cross_val.mean()}\n",
    "\n",
    "# Print results\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Accuracy: {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"  Cross-Validation Score: {metrics['Cross-Validation Score']:.4f}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
