{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation\n",
    "- Multifractal analysis (1ere approche)\n",
    "\n",
    "- Discrete Fourier Transform (DFT) $\\checkmark$\n",
    "- Spectrogram\n",
    "- Autoregression $\\checkmark$\n",
    "- Shannon encoding $\\checkmark$\n",
    "- Wavelets (en cours)\n",
    "\n",
    "- Local symbolic features\n",
    "- SAX representation\n",
    "- Approximate entropy\n",
    "\n",
    "ML\n",
    "\n",
    "- Autoencoder\n",
    "\n",
    "- RNN\n",
    "- LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install import_ipynb\n",
    "# %pip install  --user git+https://github.com/neurospin/pymultifracs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pywt\n",
    "from statsmodels.tsa.ar_model import AutoReg, ar_select_order\n",
    "import pymultifracs.mfa as mfa\n",
    "from pymultifracs.utils import build_q_log\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_batches(array, m):\n",
    "    \"\"\"\n",
    "    Transform an array of shape (n, p) into a list of arrays of shape (m, p).\n",
    "    \"\"\"\n",
    "    array = np.asarray(array)\n",
    "    p = array.shape[-1]\n",
    "    num_batches = p // m\n",
    "    return [array[i * m:(i + 1) * m] for i in range(num_batches)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=32):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim,128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, hidden_dim),\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def fit(self,X):\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        n_batch = 32\n",
    "        # Create a DataLoader for the dataset\n",
    "        X_tensor = torch.FloatTensor(X)\n",
    "        dataset = TensorDataset(X_tensor, X_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "        # Train the autoencoder\n",
    "        num_epochs = 50\n",
    "        for epoch in range(num_epochs):\n",
    "            for data in dataloader:\n",
    "                inputs, _ = data\n",
    "                inputs = inputs #.to(device)\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, inputs)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityTransform:\n",
    "    @staticmethod\n",
    "    def transform(X, **kwargs):\n",
    "        return X\n",
    "\n",
    "class Mean:\n",
    "    @staticmethod\n",
    "    def transform(X,**kwargs):\n",
    "        return np.expand_dims(X.mean(axis=-1),axis=-1)\n",
    "    \n",
    "class StandardDeviation:\n",
    "    @staticmethod\n",
    "    def transform(X,**kwargs):\n",
    "        return np.expand_dims(X.std(axis=-1),axis=-1)\n",
    "class FourierTransform:\n",
    "    @staticmethod\n",
    "    def transform(X, new_dimension=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Return the fourier transform of size 'new_dimension'.\n",
    "        \"\"\"\n",
    "        fourier_transform = np.fft.fft(X, n=new_dimension,axis=-1)\n",
    "        modulus = np.abs(fourier_transform)\n",
    "        return modulus\n",
    "\n",
    "class LowFourierTransform:\n",
    "    @staticmethod\n",
    "    def transform(X, fs=250, cutoff_min=.3, cutoff_max = 4.5, n=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Fourier transform with a cutoff_ratio.\n",
    "\n",
    "        fs: sampling frequency\n",
    "        [cutoff_min, cutoff_max]: frequency interval that we consider\n",
    "        n: new_dimension before cutoff\n",
    "\n",
    "        Return the cutoff fourier transform.\n",
    "        \"\"\"\n",
    "        fourier_transform = np.fft.fft(X,n=n,axis=-1)\n",
    "        if n is None:\n",
    "            n = X.shape[1]\n",
    "        frequencies = np.fft.fftfreq(n, d=1/fs)\n",
    "        mask = (cutoff_min < frequencies) & (frequencies < cutoff_max)\n",
    "        fourier_transform = fourier_transform[:, mask]\n",
    "        modulus = np.abs(fourier_transform)\n",
    "        return modulus\n",
    "\n",
    "class LowPsdTransform:\n",
    "    @staticmethod\n",
    "    def transform(X, fs=250, cutoff_min=.3, cutoff_max = 4.5, n=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Power Spectrum Density with a cutoff_ratio.\n",
    "\n",
    "        fs: sampling frequency\n",
    "        [cutoff_min, cutoff_max]: frequency interval that we consider\n",
    "        n: new_dimension before cutoff\n",
    "\n",
    "        Return the cutoff power spectrum density (the fourier transform of the autocorrelation).\n",
    "        \"\"\"\n",
    "        fourier_transform = np.fft.fft(X, n=n, axis=-1)\n",
    "        if n is None:\n",
    "            n = X.shape[1]\n",
    "        frequencies = np.fft.fftfreq(n, d=1/fs)\n",
    "        mask = (cutoff_min < frequencies) & (frequencies < cutoff_max)\n",
    "        fourier_transform = fourier_transform[:, mask]\n",
    "        psd = fourier_transform * np.conj(fourier_transform)\n",
    "        return psd.real\n",
    "\n",
    "class WaveDecTransform:\n",
    "    @staticmethod\n",
    "    def transform(X, level=4, wavelet='db1', mode='symmetric', **kwargs):\n",
    "        \"\"\"\n",
    "        Multilevel decomposition\n",
    "        \"\"\"\n",
    "        array = np.array(X)\n",
    "        coeffs = pywt.wavedec(array, wavelet, mode=mode, level=level)\n",
    "        coeffs_torch = [torch.tensor(c) for c in coeffs[:1]]\n",
    "        return torch.cat(coeffs_torch, dim=-1)\n",
    "\n",
    "class DwtTransform:\n",
    "    @staticmethod\n",
    "    def transform(X, wavelet='db1', mode='symmetric', **kwargs):\n",
    "        \"\"\"\n",
    "        Single level decomposition (discrete)\n",
    "        \"\"\"\n",
    "        array = np.array(X)\n",
    "        coeffs = pywt.dwt(array, wavelet, mode=mode)\n",
    "        coeffs_torch = [torch.tensor(c) for c in coeffs[:1]]\n",
    "        return torch.cat(coeffs_torch, dim=-1)\n",
    "    \n",
    "class CwtTransform:\n",
    "    @staticmethod\n",
    "    def transform(X, scales = np.arange(1,100,20), wavelet='gaus1', pca_components = 10, **kwargs):\n",
    "        \"\"\"\n",
    "        Continuous Wavelet Transform followed by PCA\n",
    "        \"\"\"\n",
    "        array = np.array(X)\n",
    "        coeffs, freqs = pywt.cwt(array,scales= scales, wavelet=wavelet)\n",
    "        \n",
    "        coeffs = np.transpose(coeffs,(1,0,2) )\n",
    "        \n",
    "        mean_coefs = np.mean(np.abs(coeffs), axis=1)\n",
    "        std_coefs = np.std(np.abs(coeffs), axis=1)\n",
    "        features = np.concatenate((mean_coefs, std_coefs),axis=-1)\n",
    "        \n",
    "        pca = PCA(n_components=pca_components) \n",
    "        X_pca = pca.fit_transform(features)\n",
    "        return X_pca\n",
    "    \n",
    "class AutoRegTransform:\n",
    "    @staticmethod\n",
    "    def get_ar_coefficients(X, k=3, **kwargs):\n",
    "        \"\"\"\n",
    "        Perform an autoregression on each example, the coefficient of the autoregression are used in the representation of the signal.\n",
    "        \n",
    "        k: the number of coefficients computed\n",
    "        \"\"\"\n",
    "        n, p = X.shape\n",
    "        X = np.array(X)\n",
    "        ar_coefficients = np.zeros((n, k))\n",
    "        for i in range(n):\n",
    "            model = AutoReg(X[i], lags=k).fit()\n",
    "            ar_coefficients[i] = model.params[1:k+1]\n",
    "        return ar_coefficients\n",
    "\n",
    "    @staticmethod\n",
    "    def transform(X, k=3, **kwargs):\n",
    "        return AutoRegTransform.get_ar_coefficients(X, k)\n",
    "\n",
    "class ShannonEncodingTransform:\n",
    "    @staticmethod\n",
    "    def transform(X, level=4, wavelet='db1', mode='symmetric', **kwargs):\n",
    "        '''\n",
    "        Inspired by https://fr.mathworks.com/help/wavelet/ug/ecg-classification-using-wavelet-features.html\n",
    "\n",
    "        -> ECG Classification Using Wavelet Packet Entropy and Random Forests (https://www.mdpi.com/1099-4300/18/8/285)\n",
    "\n",
    "        '''\n",
    "        def compute_shannon_entropy(signal):\n",
    "            return -np.nansum(signal**2 * np.log(signal**2))\n",
    "        \n",
    "        n_examples = X.shape[0]\n",
    "        wp = pywt.WaveletPacket(X[0, :], wavelet=\"sym8\", maxlevel=3)\n",
    "        packet_names = [node.path for node in wp.get_level(3, \"natural\")]\n",
    "        \n",
    "        feature_matrix_wav_packet_entropy = np.full((n_examples, 8), np.nan)\n",
    "        for i in range(len(X)):\n",
    "            wp = pywt.WaveletPacket(X[i, :], wavelet=\"sym8\", maxlevel=3)\n",
    "            for j in range(len(packet_names)):\n",
    "                new_wp = pywt.WaveletPacket(data=None, wavelet=\"sym8\", maxlevel=3)\n",
    "                new_wp[packet_names[j]] = wp[packet_names[j]].data\n",
    "                reconstructed_signal = new_wp.reconstruct(update=False)\n",
    "                feature_matrix_wav_packet_entropy[i, j] = compute_shannon_entropy(reconstructed_signal)\n",
    "        return feature_matrix_wav_packet_entropy\n",
    "\n",
    "class WaveletLeadersTransform:\n",
    "    @staticmethod\n",
    "    def transform(X, j1=2, j2=4, **kwargs):\n",
    "        '''\n",
    "        Use the multifractal framework.\n",
    "        The representation obtained is inspired by https://fr.mathworks.com/help/wavelet/ug/ecg-classification-using-wavelet-features.html:\n",
    "        [ width of the singularity spectrum = degree of multifractality, log_cumulants[1]]\n",
    "        \n",
    "        '''\n",
    "        n = X.shape[0] if X.ndim > 1 else 1\n",
    "        transformed_X = -np.ones((n, 2))\n",
    "        for i in range(X.shape[0]):\n",
    "            # print(i)\n",
    "            dwt, lwt = mfa.mf_analysis_full(\n",
    "                X[i],\n",
    "                scaling_ranges=[(j1, j2)],\n",
    "                q=build_q_log(1, 10, 20),\n",
    "                n_cumul=2,\n",
    "                p_exp=np.inf,\n",
    "                gamint=0.0\n",
    "            )\n",
    "            sf, cumul, mfs, hmin = lwt\n",
    "            transformed_X[i, :] = sf.H.item(), cumul.log_cumulants[1].item()\n",
    "        return transformed_X\n",
    "\n",
    "class MultiFracsTransform:\n",
    "    @staticmethod\n",
    "    def transform(X, j1=2, j2=4, **kwargs):\n",
    "        '''\n",
    "        Use the multifractal framework.\n",
    "        The representation obtained is inspired by HRVMultiScaling_ivanov1999:\n",
    "        [degree of multifractality, tau(3), X[i].std()]\n",
    "        where tau(3) is characterizing the scaling of the third moment Z_3(a).\n",
    "        '''\n",
    "        n = X.shape[0] if X.ndim > 1 else 1\n",
    "        transformed_X = -np.ones((n, 3))\n",
    "        for i in range(X.shape[0]):\n",
    "            dwt, lwt = mfa.mf_analysis_full(\n",
    "                X[i],\n",
    "                scaling_ranges=[(j1, j2)],\n",
    "                q=build_q_log(1, 10, 20),\n",
    "                n_cumul=2,\n",
    "                p_exp=np.inf,\n",
    "                gamint=0.0\n",
    "            )\n",
    "            sf, cumul, mfs, hmin = lwt\n",
    "            tau = interp1d(sf.q, sf.zeta[:,0,0], kind='linear')\n",
    "            degree_of_multifractality = np.max(mfs.hq) - np.min(mfs.hq)\n",
    "            transformed_X[i, :] = degree_of_multifractality, tau(3).item(), X[i].std()\n",
    "        return transformed_X\n",
    "\n",
    "class CrossCorTransform:\n",
    "    @staticmethod\n",
    "    def transform(X, new_dim=10, **kwargs):\n",
    "        \"\"\"\n",
    "        Compute the cross covariance between examples and then do PCA.\n",
    "        \"\"\"\n",
    "        crosscor = np.corrcoef(X)\n",
    "\n",
    "        pca = PCA(n_components=new_dim)\n",
    "        transformed_data = pca.fit_transform(crosscor)\n",
    "        return transformed_data# , pca.components_\n",
    "        \n",
    "        # eigen_values, eigen_vectors = np.linalg.eigh(crosscor)\n",
    "        \n",
    "        # return transformed_X\n",
    "\n",
    "class AutoCorTransform:\n",
    "    @staticmethod\n",
    "    def transform(X, k=4, m=10, **kwargs):\n",
    "        \"\"\"\n",
    "        Weird improvisation, does not work\n",
    "        \"\"\"\n",
    "        \n",
    "        n = X.shape[0] if X.ndim > 1 else 1\n",
    "        transformed_X = list()\n",
    "        for i in range(X.shape[0]):\n",
    "            batches = reshape_to_batches(X[i],m)\n",
    "            \n",
    "            autocor = np.corrcoef(batches)\n",
    "            # print(autocor.shape)\n",
    "            pca = PCA(n_components=k)\n",
    "            transformed_data = pca.fit_transform(autocor)\n",
    "            transformed_X.append(pca.components_.flatten())\n",
    "            \n",
    "        return np.array(transformed_X)\n",
    "    \n",
    "class AutoEncoderTransform:\n",
    "    @staticmethod\n",
    "    def transform(X, mode = 'n-pretrained', fourier_transform = False, n_fourier = 128, dim_encoder = 16, **kwargs):\n",
    "        \"\"\"\n",
    "        AutoEncoder Transform\n",
    "        \"\"\"\n",
    "        n = 200\n",
    "        if mode == 'pretrained':\n",
    "            fX = torch.fft.fft(torch.FloatTensor(X),n = n)\n",
    "            X = (fX  * torch.conj(fX)).real\n",
    "            autoencoder = AutoEncoder(n,10)\n",
    "            autoencoder.load_state_dict(torch.load('autoencoder_state_dict.pth'))\n",
    "            autoencoder.eval()  # Set the model to evaluation mode\n",
    "            return autoencoder.encoder(X).detach().numpy()\n",
    "        \n",
    "        else:\n",
    "            if fourier_transform:# PSD  \n",
    "                fX = torch.fft.fft(torch.FloatTensor(X), n=n_fourier)\n",
    "                X = (fX  * torch.conj(fX)).real\n",
    "            X = torch.FloatTensor(X)\n",
    "            n = X.shape[-1]\n",
    "            autoencoder = AutoEncoder(n,dim_encoder)\n",
    "            autoencoder.fit(X)\n",
    "            autoencoder.eval()  # Set the model to evaluation mode\n",
    "            return autoencoder.encoder(X).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformationRegistry:\n",
    "    def __init__(self):\n",
    "        self.transformations = {}\n",
    "\n",
    "    def register(self, name, transform_class):\n",
    "        self.transformations[name] = transform_class\n",
    "\n",
    "    def get(self, name):\n",
    "        transform_class = self.transformations.get(name)\n",
    "        if transform_class is None:\n",
    "            raise ValueError(f\"Transformation {name} not recognized.\")\n",
    "        return transform_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransform:\n",
    "    def __init__(self, registry, save_data = True,verbose = True):\n",
    "        self.registry = registry\n",
    "\n",
    "        self.save_data = save_data\n",
    "        self.data = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_trans_kwargs(input):\n",
    "        '''\n",
    "        input: [str, dict] or [str] or str\n",
    "        Return: str, dict or str, {}\n",
    "        '''\n",
    "        if isinstance(input, list):\n",
    "            if len(input) == 2 and isinstance(input[0], str) and isinstance(input[1], dict):\n",
    "                return input[0], input[1]\n",
    "            elif len(input) == 1 and isinstance(input[0], str):\n",
    "                return input[0], {}\n",
    "            else:\n",
    "                raise ValueError(\"Invalid list input. Expected [str, dict] or [str].\")\n",
    "        elif isinstance(input, str):\n",
    "            return input, {}\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input type. Expected [str, dict], [str], or str.\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_trans_kwargs_str(trans_name, kwargs):\n",
    "        def str_dict(dictionnary):\n",
    "            res = ''\n",
    "            for key,value in dictionnary.items():\n",
    "                res+=f'{key}={value}_'\n",
    "            return res\n",
    "        return (trans_name + '_' + str_dict(kwargs) )[:-1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_full_trans_kwargs_str(transformation):\n",
    "        # print(transformation)\n",
    "        res =''\n",
    "        transformation = DataTransform.sanitize_transformations(transformation)\n",
    "        for trans in transformation:\n",
    "            res += DataTransform.get_trans_kwargs_str(*DataTransform.handle_trans_kwargs(trans))+'_'\n",
    "        \n",
    "        return res[:-1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def sanitize_transformations(transformations):\n",
    "        if isinstance(transformations, str):\n",
    "            sanitized_transformations = [[transformations]]\n",
    "        elif isinstance(transformations, list) and len(transformations) == 2 and isinstance(transformations[0], str) and isinstance(transformations[1], dict):\n",
    "            sanitized_transformations = [transformations]\n",
    "\n",
    "        elif isinstance(transformations, list):\n",
    "            sanitized_transformations = []\n",
    "            for item in transformations:\n",
    "                if isinstance(item, str):\n",
    "                    sanitized_transformations.append([item])\n",
    "                elif isinstance(item, list) and len(item) == 2 and isinstance(item[0], str) and isinstance(item[1], dict):\n",
    "                    sanitized_transformations.append(item)\n",
    "                elif isinstance(item, list) and len(item) == 1 and isinstance(item[0], str):\n",
    "                    sanitized_transformations.append(item)\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid list input. Expected [str, dict] or [str].\")\n",
    "        return sanitized_transformations\n",
    "    \n",
    "    def apply_transformation(self, X, transformations):      #, **kwargs):\n",
    "        '''\n",
    "        X: data\n",
    "        transformations: [str, [str, dict], [str], ...] or [str] or str\n",
    "        '''\n",
    "        # if isinstance(transformations, str):\n",
    "        #     sanitized_transformations = [[transformations]]\n",
    "        # elif isinstance(transformations, list) and len(transformations) == 2 and isinstance(transformations[0], str) and isinstance(transformations[1], dict):\n",
    "        #     sanitized_transformations = [transformations]\n",
    "\n",
    "        # elif isinstance(transformations, list):\n",
    "        #     sanitized_transformations = []\n",
    "        #     for item in transformations:\n",
    "        #         if isinstance(item, str):\n",
    "        #             sanitized_transformations.append([item])\n",
    "        #         elif isinstance(item, list) and len(item) == 2 and isinstance(item[0], str) and isinstance(item[1], dict):\n",
    "        #             sanitized_transformations.append(item)\n",
    "        #         elif isinstance(item, list) and len(item) == 1 and isinstance(item[0], str):\n",
    "        #             sanitized_transformations.append(item)\n",
    "        #         else:\n",
    "        #             raise ValueError(\"Invalid list input. Expected [str, dict] or [str].\")\n",
    "    \n",
    "        sanitized_transformations = DataTransform.sanitize_transformations(transformations)\n",
    "        transformed_parts = []\n",
    "        for transformation in sanitized_transformations:\n",
    "            trans_name, kwargs = DataTransform.handle_trans_kwargs(transformation)\n",
    "            trans_name_kwargs = DataTransform.get_trans_kwargs_str(trans_name, kwargs)\n",
    "\n",
    "            if self.save_data and (trans_name_kwargs in self.data.keys()):\n",
    "                transformed_parts.append(self.data[trans_name_kwargs])\n",
    "            else:\n",
    "                transform_class = self.registry.get(trans_name)\n",
    "                print(f'Computing  {trans_name_kwargs} ...', end='', flush=True)\n",
    "                transformed_part = transform_class.transform(X, **kwargs)\n",
    "                print('\\r' + ' ' * len(f'Computing  {trans_name} ...') + '\\r', end='')\n",
    "                self.data[trans_name_kwargs] = transformed_part\n",
    "                transformed_parts.append(transformed_part)\n",
    "\n",
    "        return np.concatenate(transformed_parts, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize the registry\n",
    "    registry = TransformationRegistry()\n",
    "\n",
    "    # Register transformations\n",
    "    registry.register('identity', IdentityTransform)\n",
    "    registry.register('mean', Mean)\n",
    "    registry.register('std', StandardDeviation)\n",
    "    registry.register('fourier', FourierTransform)\n",
    "    registry.register('low_fourier', LowFourierTransform)\n",
    "    registry.register('low_psd', LowPsdTransform)\n",
    "    registry.register('wavedec', WaveDecTransform)\n",
    "    registry.register('dwt', DwtTransform)\n",
    "    registry.register('cwt', CwtTransform)\n",
    "    registry.register('autoreg', AutoRegTransform)\n",
    "    registry.register('shannon_encoding', ShannonEncodingTransform)\n",
    "    registry.register('wavelet_leaders', WaveletLeadersTransform)\n",
    "    registry.register('multifracs', MultiFracsTransform)\n",
    "    registry.register('crosscor', CrossCorTransform)\n",
    "    registry.register('autocor', AutoCorTransform)\n",
    "    registry.register('autoencoder', AutoEncoderTransform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation: identity, Shape: (100, 150)\n",
      "Transformation: mean, Shape: (100, 1)\n",
      "Transformation: std, Shape: (100, 1)\n",
      "Transformation: fourier, Shape: (100, 150)\n",
      "Transformation: low_fourier, Shape: (100, 2)\n",
      "Transformation: low_psd, Shape: (100, 2)\n",
      "Transformation: wavedec, Shape: (100, 10)\n",
      "Transformation: dwt, Shape: (100, 75)\n",
      "Transformation: cwt, Shape: (100, 10)\n",
      "Transformation: autoreg, Shape: (100, 3)\n",
      "Transformation: shannon_encoding, Shape: (100, 8)\n",
      "Transformation: wavelet_leaders, Shape: (100, 2)\n",
      "Transformation: multifracs, Shape: (100, 3)\n",
      "Transformation: crosscor, Shape: (100, 10)\n",
      "Transformation: autocor, Shape: (100, 60)\n",
      "Transformation: autoencoder, Shape: (100, 16)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize the data transformer\n",
    "    data_transformer = DataTransform(registry)\n",
    "\n",
    "    # Example input data\n",
    "    X = np.random.randn(100, 150)  # Example input data\n",
    "    y = np.random.randint(0, 2, 100)  # Example labels\n",
    "    \n",
    "    for trans_names in registry.transformations.keys():\n",
    "        trans_names_str = [str(name) for name in trans_names]\n",
    "        trans_name_str = '+'.join(trans_names_str) if isinstance(trans_names, list) else trans_names\n",
    "        kwargs = trans_names[1] if isinstance(trans_names, list) and len(trans_names) > 1 else {}\n",
    "        trans_names = trans_names[0] if isinstance(trans_names, list) else trans_names\n",
    "        \n",
    "        # Apply transformation\n",
    "        transformed_X = data_transformer.apply_transformation(X, trans_names, **kwargs)\n",
    "        \n",
    "        print(f\"Transformation: {trans_name_str}, Shape: {transformed_X.shape}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation: identity, Classifier: SVM, Mean Accuracy: 0.570, Std Dev: 0.051\n",
      "Transformation: identity, Classifier: DecisionTree, Mean Accuracy: 0.550, Std Dev: 0.095\n",
      "Transformation: identity, Classifier: RandomForest, Mean Accuracy: 0.540, Std Dev: 0.132\n",
      "Transformation: crosscor, Classifier: SVM, Mean Accuracy: 0.620, Std Dev: 0.103\n",
      "Transformation: crosscor, Classifier: DecisionTree, Mean Accuracy: 0.590, Std Dev: 0.136\n",
      "Transformation: crosscor, Classifier: RandomForest, Mean Accuracy: 0.640, Std Dev: 0.080\n",
      "Transformation: autocor_m=4_k=4, Classifier: SVM, Mean Accuracy: 0.560, Std Dev: 0.037\n",
      "Transformation: autocor_m=4_k=4, Classifier: DecisionTree, Mean Accuracy: 0.480, Std Dev: 0.108\n",
      "Transformation: autocor_m=4_k=4, Classifier: RandomForest, Mean Accuracy: 0.480, Std Dev: 0.040\n",
      "Transformation: fourier_crosscor, Classifier: SVM, Mean Accuracy: 0.520, Std Dev: 0.060\n",
      "Transformation: fourier_crosscor, Classifier: DecisionTree, Mean Accuracy: 0.510, Std Dev: 0.136\n",
      "Transformation: fourier_crosscor, Classifier: RandomForest, Mean Accuracy: 0.410, Std Dev: 0.111\n",
      "Transformation: wavedec, Classifier: SVM, Mean Accuracy: 0.530, Std Dev: 0.172\n",
      "Transformation: wavedec, Classifier: DecisionTree, Mean Accuracy: 0.480, Std Dev: 0.129\n",
      "Transformation: wavedec, Classifier: RandomForest, Mean Accuracy: 0.500, Std Dev: 0.164\n",
      "Transformation: autoreg_k=3, Classifier: SVM, Mean Accuracy: 0.540, Std Dev: 0.058\n",
      "Transformation: autoreg_k=3, Classifier: DecisionTree, Mean Accuracy: 0.550, Std Dev: 0.071\n",
      "Transformation: autoreg_k=3, Classifier: RandomForest, Mean Accuracy: 0.550, Std Dev: 0.045\n",
      "Transformation: identity, Classifier: SVM, Mean Accuracy: 0.570, Std Dev: 0.051\n",
      "Transformation: identity, Classifier: DecisionTree, Mean Accuracy: 0.550, Std Dev: 0.095\n",
      "Transformation: identity, Classifier: RandomForest, Mean Accuracy: 0.540, Std Dev: 0.132\n",
      "Transformation: crosscor, Classifier: SVM, Mean Accuracy: 0.620, Std Dev: 0.103\n",
      "Transformation: crosscor, Classifier: DecisionTree, Mean Accuracy: 0.590, Std Dev: 0.136\n",
      "Transformation: crosscor, Classifier: RandomForest, Mean Accuracy: 0.640, Std Dev: 0.080\n",
      "Transformation: autocor_m=4_k=4, Classifier: SVM, Mean Accuracy: 0.560, Std Dev: 0.037\n",
      "Transformation: autocor_m=4_k=4, Classifier: DecisionTree, Mean Accuracy: 0.480, Std Dev: 0.108\n",
      "Transformation: autocor_m=4_k=4, Classifier: RandomForest, Mean Accuracy: 0.480, Std Dev: 0.040\n",
      "Transformation: fourier_crosscor, Classifier: SVM, Mean Accuracy: 0.520, Std Dev: 0.060\n",
      "Transformation: fourier_crosscor, Classifier: DecisionTree, Mean Accuracy: 0.510, Std Dev: 0.136\n",
      "Transformation: fourier_crosscor, Classifier: RandomForest, Mean Accuracy: 0.410, Std Dev: 0.111\n",
      "Transformation: wavedec, Classifier: SVM, Mean Accuracy: 0.530, Std Dev: 0.172\n",
      "Transformation: wavedec, Classifier: DecisionTree, Mean Accuracy: 0.480, Std Dev: 0.129\n",
      "Transformation: wavedec, Classifier: RandomForest, Mean Accuracy: 0.500, Std Dev: 0.164\n",
      "Transformation: autoreg_k=3, Classifier: SVM, Mean Accuracy: 0.540, Std Dev: 0.058\n",
      "Transformation: autoreg_k=3, Classifier: DecisionTree, Mean Accuracy: 0.550, Std Dev: 0.071\n",
      "Transformation: autoreg_k=3, Classifier: RandomForest, Mean Accuracy: 0.550, Std Dev: 0.045\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    \n",
    "\n",
    "    # Define the classifiers to be tested\n",
    "    classifiers = {\n",
    "        'SVM': SVC(),\n",
    "        'DecisionTree': DecisionTreeClassifier(),\n",
    "        'RandomForest': RandomForestClassifier()\n",
    "    }\n",
    "\n",
    "    # Define the transformations to be tested\n",
    "    transformations = [\n",
    "        'identity',\n",
    "        ['crosscor'],\n",
    "        ['autocor',{'m':4,'k':4}],\n",
    "        [['fourier'],['crosscor']],\n",
    "        ['wavedec'],\n",
    "        ['autoreg', {'k': 3}]\n",
    "    ]\n",
    "\n",
    "    \n",
    "\n",
    "    # Function to evaluate a classifier using cross-validation\n",
    "    def evaluate_classifier_cv(classifier, X, y):\n",
    "        scores = cross_val_score(classifier, X, y, cv=5)  # 5-fold cross-validation\n",
    "        return np.mean(scores), np.std(scores)\n",
    "\n",
    "    # Loop over each transformation and each classifier\n",
    "    results = {}\n",
    "\n",
    "    for trans in transformations:\n",
    "\n",
    "        trans_name_str = DataTransform.get_full_trans_kwargs_str(trans)\n",
    "        # trans_name_str = '+'.join(trans_names_str) if isinstance(trans_names, list) else trans_names\n",
    "        # kwargs = trans_names[1] if isinstance(trans_names, list) and len(trans_names) > 1 else {}\n",
    "        # trans_names = trans_names[0] if isinstance(trans_names, list) else trans_names\n",
    "        \n",
    "        # Apply transformation\n",
    "        # print(trans)\n",
    "        transformed_X = data_transformer.apply_transformation(X, trans)\n",
    "        # Standardize the data (important for some classifiers like SVM)\n",
    "        scaler = StandardScaler()\n",
    "        transformed_X = scaler.fit_transform(transformed_X)\n",
    "        \n",
    "        results[trans_name_str] = {}\n",
    "        for clf_name, clf in classifiers.items():\n",
    "            # Evaluate the classifier with cross-validation\n",
    "            mean_accuracy, std_accuracy = evaluate_classifier_cv(clf, transformed_X, y)\n",
    "            results[trans_name_str][clf_name] = (mean_accuracy, std_accuracy)\n",
    "            print(f\"Transformation: {trans_name_str}, Classifier: {clf_name}, Mean Accuracy: {mean_accuracy:.3f}, Std Dev: {std_accuracy:.3f}\")\n",
    "\n",
    "    # Print the results\n",
    "    for trans_name, clf_results in results.items():\n",
    "        for clf_name, (mean_accuracy, std_accuracy) in clf_results.items():\n",
    "            print(f\"Transformation: {trans_name}, Classifier: {clf_name}, Mean Accuracy: {mean_accuracy:.3f}, Std Dev: {std_accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1                         ...\n",
      "Computing  multifracs_i=1 ..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\pymultifracs\\utils.py:87: RuntimeWarning: divide by zero encountered in power\n",
      "  return np.power(array, exponent)\n",
      "c:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\pymultifracs\\utils.py:76: RuntimeWarning: divide by zero encountered in reciprocal\n",
      "  return array ** exponent\n",
      "c:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\pymultifracs\\cumulants.py:265: RuntimeWarning: divide by zero encountered in log\n",
      "  log_T_X_j = np.log(T_X_j)\n",
      "c:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\pymultifracs\\mfspectrum.py:117: RuntimeWarning: invalid value encountered in divide\n",
      "  R_j = temp / Z\n",
      "c:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\pymultifracs\\mfspectrum.py:118: RuntimeWarning: divide by zero encountered in log2\n",
      "  V[:, ind_j, :] = fixednansum(R_j * np.log2(mrq_values_j), axis=1)\n",
      "c:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\pymultifracs\\mfspectrum.py:118: RuntimeWarning: invalid value encountered in multiply\n",
      "  V[:, ind_j, :] = fixednansum(R_j * np.log2(mrq_values_j), axis=1)\n",
      "c:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\pymultifracs\\mfspectrum.py:119: RuntimeWarning: divide by zero encountered in log2\n",
      "  U[:, ind_j, :] = np.log2(nj) + fixednansum((R_j * np.log2(R_j)),\n",
      "c:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\pymultifracs\\mfspectrum.py:119: RuntimeWarning: invalid value encountered in multiply\n",
      "  U[:, ind_j, :] = np.log2(nj) + fixednansum((R_j * np.log2(R_j)),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2                        \n",
      "3                         ...\n",
      "4                         ...\n",
      "5                         ...\n",
      "6                         ...\n",
      "7                         ...\n",
      "8                         ...\n",
      "9                         ...\n",
      "10                        ...\n",
      "11                       0 ...\n",
      "12                       1 ...\n",
      "13                       2 ...\n",
      "14                       3 ...\n",
      "15                       4 ...\n",
      "16                       5 ...\n",
      "17                       6 ...\n",
      "18                       7 ...\n",
      "19                       8 ...\n",
      "20                       9 ...\n",
      "21                       0 ...\n",
      "22                       1 ...\n",
      "23                       2 ...\n",
      "24                       3 ...\n",
      "25                       4 ...\n",
      "26                       5 ...\n",
      "27                       6 ...\n",
      "28                       7 ...\n",
      "29                       8 ...\n",
      "30                       9 ...\n",
      "31                       0 ...\n",
      "32                       1 ...\n",
      "33                       2 ...\n",
      "34                       3 ...\n",
      "35                       4 ...\n",
      "36                       5 ...\n",
      "37                       6 ...\n",
      "38                       7 ...\n",
      "39                       8 ...\n",
      "40                       9 ...\n",
      "41                       0 ...\n",
      "42                       1 ...\n",
      "43                       2 ...\n",
      "44                       3 ...\n",
      "45                       4 ...\n",
      "46                       5 ...\n",
      "47                       6 ...\n",
      "48                       7 ...\n",
      "49                       8 ...\n",
      "50                       9 ...\n",
      "51                       0 ...\n",
      "52                       1 ...\n",
      "53                       2 ...\n",
      "54                       3 ...\n",
      "55                       4 ...\n",
      "56                       5 ...\n",
      "57                       6 ...\n",
      "58                       7 ...\n",
      "59                       8 ...\n",
      "60                       9 ...\n",
      "61                       0 ...\n",
      "62                       1 ...\n",
      "63                       2 ...\n",
      "64                       3 ...\n",
      "65                       4 ...\n",
      "66                       5 ...\n",
      "67                       6 ...\n",
      "68                       7 ...\n",
      "69                       8 ...\n",
      "70                       9 ...\n",
      "71                       0 ...\n",
      "72                       1 ...\n",
      "73                       2 ...\n",
      "74                       3 ...\n",
      "75                       4 ...\n",
      "76                       5 ...\n",
      "77                       6 ...\n",
      "78                       7 ...\n",
      "79                       8 ...\n",
      "80                       9 ...\n",
      "Computing  multifracs_i=80 ..."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Apply transformation\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m---> 14\u001b[0m     transformed_X \u001b[38;5;241m=\u001b[39m data_transformer\u001b[38;5;241m.\u001b[39mapply_transformation(X[i,:][\u001b[38;5;28;01mNone\u001b[39;00m,:], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultifracs\u001b[39m\u001b[38;5;124m'\u001b[39m,{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m:i}])\n",
      "Cell \u001b[1;32mIn[47], line 98\u001b[0m, in \u001b[0;36mDataTransform.apply_transformation\u001b[1;34m(self, X, transformations)\u001b[0m\n\u001b[0;32m     96\u001b[0m transform_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregistry\u001b[38;5;241m.\u001b[39mget(trans_name)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComputing  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrans_name_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 98\u001b[0m transformed_part \u001b[38;5;241m=\u001b[39m transform_class\u001b[38;5;241m.\u001b[39mtransform(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComputing  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrans_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[trans_name_kwargs] \u001b[38;5;241m=\u001b[39m transformed_part\n",
      "Cell \u001b[1;32mIn[45], line 191\u001b[0m, in \u001b[0;36mMultiFracsTransform.transform\u001b[1;34m(X, j1, j2, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m transformed_X \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mones((n, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m--> 191\u001b[0m     dwt, lwt \u001b[38;5;241m=\u001b[39m mfa\u001b[38;5;241m.\u001b[39mmf_analysis_full(\n\u001b[0;32m    192\u001b[0m         X[i],\n\u001b[0;32m    193\u001b[0m         scaling_ranges\u001b[38;5;241m=\u001b[39m[(j1, j2)],\n\u001b[0;32m    194\u001b[0m         q\u001b[38;5;241m=\u001b[39mbuild_q_log(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m),\n\u001b[0;32m    195\u001b[0m         n_cumul\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    196\u001b[0m         p_exp\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39minf,\n\u001b[0;32m    197\u001b[0m         gamint\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m     sf, cumul, mfs, hmin \u001b[38;5;241m=\u001b[39m lwt\n\u001b[0;32m    200\u001b[0m     tau \u001b[38;5;241m=\u001b[39m interp1d(sf\u001b[38;5;241m.\u001b[39mq, sf\u001b[38;5;241m.\u001b[39mzeta[:,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m], kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\pymultifracs\\mfa.py:194\u001b[0m, in \u001b[0;36mmf_analysis_full\u001b[1;34m(signal, scaling_ranges, normalization, gamint, weighted, wt_name, p_exp, q, n_cumul, bootstrap_weighted, estimates, R)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wt_transform\u001b[38;5;241m.\u001b[39mwt_leaders \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m     mrq \u001b[38;5;241m=\u001b[39m [mrq, wt_transform\u001b[38;5;241m.\u001b[39mwt_leaders]\n\u001b[1;32m--> 194\u001b[0m mf_data \u001b[38;5;241m=\u001b[39m mf_analysis(\n\u001b[0;32m    195\u001b[0m     mrq,\n\u001b[0;32m    196\u001b[0m     scaling_ranges,\n\u001b[0;32m    197\u001b[0m     weighted\u001b[38;5;241m=\u001b[39mweighted,\n\u001b[0;32m    198\u001b[0m     n_cumul\u001b[38;5;241m=\u001b[39mn_cumul,\n\u001b[0;32m    199\u001b[0m     q\u001b[38;5;241m=\u001b[39mq,\n\u001b[0;32m    200\u001b[0m     bootstrap_weighted\u001b[38;5;241m=\u001b[39mbootstrap_weighted,\n\u001b[0;32m    201\u001b[0m     R\u001b[38;5;241m=\u001b[39mR,\n\u001b[0;32m    202\u001b[0m     estimates\u001b[38;5;241m=\u001b[39mestimates,\n\u001b[0;32m    203\u001b[0m )\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mf_data\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\pymultifracs\\mfa.py:64\u001b[0m, in \u001b[0;36mmf_analysis\u001b[1;34m(mrq, scaling_ranges, weighted, n_cumul, q, bootstrap_weighted, R, estimates, robust, robust_kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (n \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mlen\u001b[39m(estimates)) \u001b[38;5;241m!=\u001b[39m (m \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mlen\u001b[39m(mrq)):\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     61\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of `estimates` = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match `mrq` = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m         )\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ([mf_analysis(m, scaling_ranges, weighted, n_cumul,\n\u001b[0;32m     65\u001b[0m                          q, bootstrap_weighted, R, estimates[i], robust,\n\u001b[0;32m     66\u001b[0m                          robust_kwargs)\n\u001b[0;32m     67\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(mrq)])\n\u001b[0;32m     69\u001b[0m scaling_ranges \u001b[38;5;241m=\u001b[39m sanitize_scaling_ranges(scaling_ranges, mrq\u001b[38;5;241m.\u001b[39mj2_eff())\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scaling_ranges) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\pymultifracs\\mfa.py:64\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (n \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mlen\u001b[39m(estimates)) \u001b[38;5;241m!=\u001b[39m (m \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mlen\u001b[39m(mrq)):\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     61\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of `estimates` = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match `mrq` = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m         )\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ([mf_analysis(m, scaling_ranges, weighted, n_cumul,\n\u001b[0;32m     65\u001b[0m                          q, bootstrap_weighted, R, estimates[i], robust,\n\u001b[0;32m     66\u001b[0m                          robust_kwargs)\n\u001b[0;32m     67\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(mrq)])\n\u001b[0;32m     69\u001b[0m scaling_ranges \u001b[38;5;241m=\u001b[39m sanitize_scaling_ranges(scaling_ranges, mrq\u001b[38;5;241m.\u001b[39mj2_eff())\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scaling_ranges) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\pymultifracs\\mfa.py:117\u001b[0m, in \u001b[0;36mmf_analysis\u001b[1;34m(mrq, scaling_ranges, weighted, n_cumul, q, bootstrap_weighted, R, estimates, robust, robust_kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m struct, cumul, spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m estimates:\n\u001b[1;32m--> 117\u001b[0m     struct \u001b[38;5;241m=\u001b[39m StructureFunction\u001b[38;5;241m.\u001b[39mfrom_dict(parameters)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m estimates:\n\u001b[0;32m    119\u001b[0m     cumul \u001b[38;5;241m=\u001b[39m Cumulants\u001b[38;5;241m.\u001b[39mfrom_dict(parameters)\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\pymultifracs\\multiresquantity.py:72\u001b[0m, in \u001b[0;36mMultiResolutionQuantityBase.from_dict\u001b[1;34m(cls, d)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_dict\u001b[39m(\u001b[38;5;28mcls\u001b[39m, d):\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Method to instanciate a dataclass by passing a dictionary with\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    extra keywords\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m              parameters, similarly to introducing a \\*\\*kwargs parameter.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\n\u001b[0;32m     73\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m d\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mcls\u001b[39m)\u001b[38;5;241m.\u001b[39mparameters\n\u001b[0;32m     75\u001b[0m     })\n",
      "File \u001b[1;32m<string>:7\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, mrq, q, scaling_ranges, weighted, bootstrapped_mfa)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\pymultifracs\\structurefunction.py:90\u001b[0m, in \u001b[0;36mStructureFunction.__post_init__\u001b[1;34m(self, mrq, bootstrapped_mfa)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bootstrapped_mfa \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrapped_mrq \u001b[38;5;241m=\u001b[39m bootstrapped_mfa\u001b[38;5;241m.\u001b[39mstructure\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute(mrq)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_zeta(mrq\u001b[38;5;241m.\u001b[39mn_rep)\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\pymultifracs\\structurefunction.py:103\u001b[0m, in \u001b[0;36mStructureFunction._compute\u001b[1;34m(self, mrq)\u001b[0m\n\u001b[0;32m    100\u001b[0m     s_j \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], mrq\u001b[38;5;241m.\u001b[39mn_rep))\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ind_q, q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq):\n\u001b[1;32m--> 103\u001b[0m         s_j[ind_q, :] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmean(fast_power(np\u001b[38;5;241m.\u001b[39mabs(c_j), q), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    105\u001b[0m     values[:, ind_j, :] \u001b[38;5;241m=\u001b[39m s_j\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogvalues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog2(values)\n",
      "File \u001b[1;32mc:\\Users\\aejog\\anaconda3\\Lib\\site-packages\\pymultifracs\\utils.py:87\u001b[0m, in \u001b[0;36mfast_power\u001b[1;34m(array, exponent)\u001b[0m\n\u001b[0;32m     83\u001b[0m         array_out \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m array\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array_out\n\u001b[1;32m---> 87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mpower(array, exponent)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize the data transformer\n",
    "    data_transformer = DataTransform(registry)\n",
    "\n",
    "    # Example input data\n",
    "    data = np.load('ecgs_labels.npy')\n",
    "    X, y = data[:,:-1], data[:,-1] # Example input data\n",
    "    p = np.random.permutation(X.shape[0])\n",
    "    X = X[p,:]\n",
    "    y =y[p]\n",
    "    for i in range(X.shape[0]):\n",
    "        # Apply transformation\n",
    "        print(i)\n",
    "        transformed_X = data_transformer.apply_transformation(X[i,:][None,:], ['multifracs',{'i':i}])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
